{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpc4g0lbOvdyvFjRD1aPoc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_VII_3_TextClassExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification: Getting started\n",
        "\n",
        "\n",
        "In this tutorial, we go through a text classification example to see how text data can be used in ML applications. We rely on (a sllightly reduced version of) the **AG News Classification Dataset** (see here), which contains news articles, their titles, and what type of article it is (class ids 1-4 where 1-World, 2-Sports, 3-Business, 4-Sci/Tech).\n",
        "\n",
        "As usually, let's start with loading the relevant libaries."
      ],
      "metadata": {
        "id": "NDIUNtLS9riU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTUizURa9db7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's import the dataset:"
      ],
      "metadata": {
        "id": "d3uxSIv1_W5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "metadata": {
        "id": "mTq-bR8q_WJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "News = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB888_VII_3_TextClassExample.csv', index_col=0)"
      ],
      "metadata": {
        "id": "lNIOiz96_gIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "News.head()"
      ],
      "metadata": {
        "id": "IsGCVs9R_2Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare and Clean the Data\n",
        "\n",
        "We start by preparing the text data. We first reset the \"Index\" (because the first column really is a category) and make it a class variable:"
      ],
      "metadata": {
        "id": "53U0-udq_ujt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "News = News.reset_index()\n",
        "News = News.rename(columns={\"Class Index\": \"Category\"})\n",
        "News['Category'] = pd.Categorical(News['Category'])\n",
        "News.head()"
      ],
      "metadata": {
        "id": "OMTpdNHuTFI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's combine the Title and Description columns:"
      ],
      "metadata": {
        "id": "RQpgnhXbTlno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "News['Combined'] = News['Title'] + ' ' + News['Description']\n",
        "News = News.drop(['Title', 'Description'], axis=1)\n",
        "News.head()"
      ],
      "metadata": {
        "id": "ehSQlrMjRReM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we do a few steps to process the text into a more usable format. Let's start by **removing punctutation**:"
      ],
      "metadata": {
        "id": "CSkrf2GoRmLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "News['Combined'] = News['Combined'].apply(remove_punctuation)\n",
        "News.head()"
      ],
      "metadata": {
        "id": "5eqrmfOMTva1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's make it all **lower case**:"
      ],
      "metadata": {
        "id": "-xn5ysnSUa2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "News['Combined'] = News['Combined'].str.lower()\n",
        "News.head()"
      ],
      "metadata": {
        "id": "88LycssEUcgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's **remove \"stopwords\"**, i.e., words that don't contribute to the meaning:"
      ],
      "metadata": {
        "id": "yBTSvzpHUcQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "News['Combined'] = News['Combined'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "News.head()"
      ],
      "metadata": {
        "id": "Q1ZLnKmSVD5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's do **\"stemming\"**, i.e., let's remove \"-ly\" or \"ing\" etc."
      ],
      "metadata": {
        "id": "eVTDnDWCVXiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "News['Combined'] = News['Combined'].apply(lambda x: ' '.join([porter.stem(word) for word in x.split()]))\n",
        "News.head()"
      ],
      "metadata": {
        "id": "DJGve2WCVxE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, let's carry out **\"Lemmatization\"**, i.e., let's convert the word into root word:"
      ],
      "metadata": {
        "id": "alca_R4EWMFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "News['Combined'] = News['Combined'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "News.head()"
      ],
      "metadata": {
        "id": "LdQt1oNUVXuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the data as our \"processed\" data set:"
      ],
      "metadata": {
        "id": "ZpdGnPVBWmwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "News.to_csv('GB888_VII_3_TextClassExample_processed.csv', index=False)"
      ],
      "metadata": {
        "id": "sBzs2NKwWsKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize data\n",
        "\n",
        "We visualize the data via wordclouds characterizing the differnet categorgies. Recall the categories are  1-World, 2-Sports, 3-Business, 4-Sci/Tech:"
      ],
      "metadata": {
        "id": "Xi2YmAmKXHB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and display word cloud\n",
        "def generate_wordcloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Generate word cloud for all data\n",
        "combined_text = ' '.join(News['Combined'].tolist())\n",
        "generate_wordcloud(combined_text, \"Combined Word Cloud (All Data)\")\n",
        "\n",
        "# Generate word clouds for each category\n",
        "for category in range(1, 5):\n",
        "    category_text = ' '.join(News[News['Category'] == category]['Combined'].tolist())\n",
        "    generate_wordcloud(category_text, f\"Combined Word Cloud (Category {category})\")\n"
      ],
      "metadata": {
        "id": "B6LZ473JXQPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So they are quite different!\n",
        "\n",
        "## Encode the Text\n",
        "\n",
        "We use Tf-IDF to encode the text. This is a variant of the Bag of Words approach, where we cnsider the importance of words by assigning weights to words based on their frequency in a article and their rarity across all articles:"
      ],
      "metadata": {
        "id": "ZCw0esLZYG3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(News['Combined'])\n",
        "X.shape"
      ],
      "metadata": {
        "id": "jRPYu9rOYJbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way to think about X is that there seem to be 74389 words, and X gives the frequency of each of these words across all the words."
      ],
      "metadata": {
        "id": "vulDc_wjZtLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Modeling\n",
        "\n",
        "So let's do some ML. We start by splitting our data into a training and test set"
      ],
      "metadata": {
        "id": "wLU1H4fcZ-Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = News['Category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "Ur7tpUewZ9BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-class Regression\n",
        "\n",
        "We start by a multi-nomial regression. Note that Logistic Regression in sklearn does have multi-class support, so we just rely on that:"
      ],
      "metadata": {
        "id": "iz3Zok1RbUXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "-bgIqDNzabFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate, let's use a multi-class confusion matrix:"
      ],
      "metadata": {
        "id": "2GfDqqipbvvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[1, 2, 3, 4], yticklabels=[1, 2, 3, 4])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ddZYlaQxbDiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the overall accuracy:"
      ],
      "metadata": {
        "id": "Q7CtZ_MAcIwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "fwVZS9lnb-bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty good!\n",
        "\n",
        "### Neural Network Modeling\n",
        "\n",
        "Let's also try a simple neural network model. Since we are using keras, it's good to convert the outcomes into levels between 0 and 3 (rather than 1 to 4):"
      ],
      "metadata": {
        "id": "uM5kRiGRcN3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = News['Category'].astype(int) - 1\n",
        "y = to_categorical(y,4)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "DBnFhxRIex8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have a high-dimensional input vector, we are going with a simple network. We, again somewhat arbitrarily, build a neural network with one hidden layer with ReLu activation functions and three neurons. Since this is a multi-class classification problem we use a softmax output layer:"
      ],
      "metadata": {
        "id": "6SOSs0IJi4qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(3, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9JCeBp1FcVUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "daBpli_KdeXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we're overfitting after epoch 5 or 6...\n",
        "\n",
        "To see how we are doing, we determine the accuracy in the test set and again plot a multi-class confusion matrix"
      ],
      "metadata": {
        "id": "uxRBIHjW-F_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[1, 2, 3, 4], yticklabels=[1, 2, 3, 4])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "RHAqvW4efUVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the performance is fairly similar to the logistic regression model. We of course can work more on refining the model. However, a question is whether a simple feed-forward model using this bag of words input vector (that ignores order, grammar, and \"meaning\") is the best we can do..."
      ],
      "metadata": {
        "id": "sqVOt1M9-M7n"
      }
    }
  ]
}