{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_V_9_RunningNNsIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Artificial Neural Nets\n",
        "\n",
        "In this tutorial, we will get to know artificial neural networks for the purpose of prediction.  Neural nets present a powerful machine learning technique, but there are also various pitfalls.  In the first part of this tutorial, we will demonstrate both their power and key issues.  More precisely, we will look at two separate regression examples where their performance comes out different. This will help us understand some of the accolades and downsides.  \n",
        "\n",
        "As usually, we start by importing the relevant libraries:"
      ],
      "metadata": {
        "id": "PWMK_A-I2yI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2sMdprSo9cR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import mean_squared_error, roc_curve, auc\n",
        "import statsmodels.formula.api as smf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for later use, we defined the sigmoid function:"
      ],
      "metadata": {
        "id": "L3wRkuDH3nKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vV1PKCjo9cS"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return(1 / (1 + np.exp(-x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Networks\n",
        "\n",
        "### Review of Concepts and Maths\n",
        "\n",
        "As we discussed in lecture, a neural net generally consists of an *input layer* with the features $X_1,\\ldots,X_p,$ one or more *hidden layers* with *neurons* $Z_1,\\ldots,Z_M,$ and an *output layer*.  In the case of a one-dimensional regression problem, the output layer consists of a single outcome $Y.$  \n",
        "\n",
        "In a single-layer neural network, the inputs are processed into the neurons of the hidden layer, which in turn are processed into the outputs.  More precisely, for each neuron $Z_m$, the $X_j$'s are linearly aggregated and transformed via a sigmoid function:   \n",
        "$$\n",
        "\\boxed{\\begin{array}{rcl}\n",
        "Z_m &=& \\sigma(\\alpha_{0m} +\n",
        "\\alpha_{m,1}\\,X_1 + \\alpha_{m,2}\\,X_2 + \\ldots + \\alpha_{m,p}\\,X_p)\\\\\n",
        "Y &=& \\beta_0 + \\beta_1\\,Z_1 + \\beta_2\\,Z_2 + \\ldots + \\beta_M\\,Z_M + \\varepsilon\n",
        "\\end{array}}\n",
        "$$\n",
        "The sigmoid function has the appealing property that it can depict highly linear and highly non-linear relationships.  The constant term ($\\alpha_{0m}$) together with the norm of the coefficient vector $(\\alpha_{m1},\\ldots,\\alpha_{mp})$ determines how (non-)linear the relationship in that neuron is.  The neurons are then aggregated to the response $Y.$  In a *deep* neural network, there are several hidden layers in which the neurons are processed into other neurons.\n",
        "\n",
        "As a consequence, the number of parameters in a neural network quickly becomes large:  For instance, in a single-layer network with a one-dimensional response, we have *weights* for each neuron ($\\{\\alpha_{0m},\\alpha_{m},m=1,\\ldots,M\\}$), amounting to $M \\times (p+1)$ parameters, and then another $M+1$ parameters in producing $Y$ ($\\{\\beta_0,\\beta\\}$).  The number increases drastically for more hidden layers.\n",
        "\n",
        "### Simulated Example I: Complex Relationship, No Noise, Lots of Data\n",
        "\n",
        "We start with a rather complex relationship between inputs and outputs but assume there is no random noise ($\\varepsilon$ is zero) and that we are in a data-rich environment:\n"
      ],
      "metadata": {
        "id": "sfj9a25u3Zhr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btt5li3No9cS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "x1 = np.random.normal(0, 1, 20000)\n",
        "x2 = np.random.normal(0, 1, 20000)\n",
        "y = (x1+0.5)*(x1+0.5)*(x1+0.5)*(x2-0.3)*(x2-0.3)+50*1/(1+np.exp(3*x1+2*x2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3xPvn56o9cS"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 12))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, y, c='k', marker='o')\n",
        "\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we observe that the relationship is relatively complex in both variables.  \n"
      ],
      "metadata": {
        "id": "UAOSPwZY31Xt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's zoom in a bit:"
      ],
      "metadata": {
        "id": "aVIsMMhA36zW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxSvkOCZo9cT"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 12))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, y, c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-20,80)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if a neural network can fit the relationship.\n",
        "\n",
        "There are multiple choices for neuralnetwork packages in Python. The one from scikit has similar setting the one in R. Here we rely on keras in tensorflow, which is a package that can do heavy lifting.\n",
        "\n",
        "We start by putting together the dataframe..."
      ],
      "metadata": {
        "id": "uFCh_x804hwk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgL3SunZo9cT"
      },
      "outputs": [],
      "source": [
        "mydata1 = pd.DataFrame({'y':y,'x1':x1,'x2':x2})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and scale the data:"
      ],
      "metadata": {
        "id": "2Ib2GmjA47AV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u80I0Y4fo9cT"
      },
      "outputs": [],
      "source": [
        "mydata1_sc = scale(mydata1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4WGkt96o9cU"
      },
      "outputs": [],
      "source": [
        "X_train = mydata1_sc[:,1:3]\n",
        "y_train = mydata1_sc[:,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a simple network with two layers and three neurons in the first layer and 3 neurons in the second layer:"
      ],
      "metadata": {
        "id": "UF1CjSt65LZA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reIkfKDro9cU"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(2,)) # we have a two-dimensional input layer\n",
        "x = layers.Dense(5, activation=\"sigmoid\", name=\"dense_1\")(inputs) #the next layer takes the inputs and pushes them into 5 neurons; we use signoid activation\n",
        "z = layers.Dense(3, activation=\"sigmoid\", name=\"dense_2\")(x) #the next layer takes the 5 neurons as input and pushes them into 3 neurons; again we use sigmoid activation\n",
        "outputs = layers.Dense(1, activation=\"linear\", name=\"predictions\")(z) #the output is single dimension\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f5OimXEo9cU"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),  # Optimize\n",
        "    loss=keras.losses.MeanSquaredError(),  # Loss function to minimize\n",
        "    metrics=[keras.metrics.MeanSquaredError()], # List of metrics to monitor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m8nMRAV_o9cU"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    # batch_size=64,\n",
        "    epochs=75\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the predictions:"
      ],
      "metadata": {
        "id": "zHNojiL-6-2P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlmIxDyGo9cU"
      },
      "outputs": [],
      "source": [
        "pred_sc = model.predict(X_train)\n",
        "preds_1 = pred_sc * np.std(mydata1['y']) + np.mean(mydata1['y']) #WE rescale up\n",
        "compare = pd.DataFrame({'y':y,'preds_1':preds_1.T[0]})\n",
        "compare.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we get pretty close:"
      ],
      "metadata": {
        "id": "m1bN_rPZ7DBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5xXt2XLo9cU"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(16, 12))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, preds_1.T[0], c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-20,80)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, based on the first few predictions, the network seems to work quite well. The plot demonstrates that the neural net fit is able to depict the relationship quite adequately.   \n",
        "\n",
        "### Simulated Example II: Simple Relationship, Lots of Noise, Little Data\n",
        "\n",
        "Let's continue with a second example, where the relationship is a lot simpler, the dataset is smaller, and where there is significant noise -- so that the *signal-to-noise ratio* is relatively low."
      ],
      "metadata": {
        "id": "3Oct5zjF7PgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYrJy9WVo9cV"
      },
      "outputs": [],
      "source": [
        "np.random.seed(3)\n",
        "x1 = np.random.normal(0, 1, 100)\n",
        "x2 = np.random.normal(0, 1, 100)\n",
        "x3 = np.random.normal(0, 1, 100)\n",
        "y2_true = 3 + 2 * x1 + 4 * x2\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, y2_true, c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-10,20)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul-e4Azpo9cV"
      },
      "outputs": [],
      "source": [
        "y2 = y2_true + 5 * np.random.normal(0, 1, 100)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, y2, c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-20,30)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt0xGiIQo9cV"
      },
      "outputs": [],
      "source": [
        "mydata2 = pd.DataFrame({'y2':y2,'x1':x1,'x2':x2,'x3':x3})\n",
        "mydata2_sc = scale(mydata2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's again run a two-layer neural network:"
      ],
      "metadata": {
        "id": "uhvRsZW_8q7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiftsllKo9cV"
      },
      "outputs": [],
      "source": [
        "X_train = mydata2_sc[:,1:4]\n",
        "y_train = mydata2_sc[:,0]\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "x = layers.Dense(5, activation=\"sigmoid\", name=\"dense_1\")(inputs)\n",
        "z = layers.Dense(3, activation=\"sigmoid\", name=\"dense_2\")(x)\n",
        "outputs = layers.Dense(1, activation=\"linear\", name=\"predictions\")(z)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.01),  # Optimizer\n",
        "    # Loss function to minimize\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    # List of metrics to monitor\n",
        "    metrics=[keras.metrics.MeanSquaredError()],\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    # batch_size=64,\n",
        "    epochs=50\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's generate predictions:"
      ],
      "metadata": {
        "id": "igpX1M4G9Xig"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc6YHC8io9cV"
      },
      "outputs": [],
      "source": [
        "pred2_sc = model.predict(X_train)\n",
        "preds_2 = pred2_sc*np.std(mydata2['y2']) + np.mean(mydata2['y2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVPeIQ0No9cV"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, preds_2, c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-20,30)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GCj1RZio9cW"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(x1, x2, y2_true-preds_2.T[0], c='k', marker='o')\n",
        "\n",
        "ax.set_xlim3d(-2,2)\n",
        "ax.set_ylim3d(-2,2)\n",
        "ax.set_zlim3d(-20,30)\n",
        "ax.set_xlabel('x1 Label')\n",
        "ax.set_ylabel('x2 Label')\n",
        "ax.set_zlabel('y Label')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUQxkyXEo9cW"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(y2_true, preds_2.T[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare to a simple linear regression model:"
      ],
      "metadata": {
        "id": "I7USOu8d_Heq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhmLbLHo9cW"
      },
      "outputs": [],
      "source": [
        "lmfit = smf.ols(formula=\"y2 ~ x1 + x2 + x3\", data=mydata2).fit()\n",
        "yhat_OOS = lmfit.predict(mydata2.drop(columns = [\"y2\"]))\n",
        "print(lmfit.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cooF9Zqso9cW"
      },
      "outputs": [],
      "source": [
        "mean_squared_error(y2_true, yhat_OOS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, even though the relationship between the inputs and the (non-noisy) outputs is relatively simple, the neural network predicts a more complex one.  The residuals (relative to the non-noisy outcomes) are quite sizable relative to the data and the predictions.  Clearly, the neural network is overfitting the data.\n",
        "\n",
        "For the OLS regression, the prediction error is low.  Of course, this may not be suprising given that the original relationship corresponds to a linear model -- but that's the point.  A simpler model will outperform the more advanced model if it is able to accurately depict the relationship!"
      ],
      "metadata": {
        "id": "LPjP0RUs_BOJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}