{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB886_III_7_GammaRegressionExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gamma Regression\n",
        "\n",
        "We consider a dataset that has miles-per-gallon for caes based on horse-power, acceleration, and car origin (we take an excerpt from the [UCI dataset mpg](https://archive.ics.uci.edu/dataset/9/auto+mpg]) that has various other features). Our objective is to predict current mpg. You can imagine a use-case as checking reported mpg figures by producers and whether a care is efficient relative to its characteristics.\n",
        "\n",
        "We will first use OLS linear regression, illustrate potential issues, and then showcase Gamma regression as an alternative."
      ],
      "metadata": {
        "id": "gZu26MYxhcnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the libraries that are going to be helpful. We're again going to rely on the package [statsmodels](https://www.statsmodels.org/stable/index.html) and the statistical learning toolkit [ski-cit learn](https://scikit-learn.org/stable/index.html), which provide GLM functionalty."
      ],
      "metadata": {
        "id": "LpmpLD0WhMNi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL8OZzdpMbDG"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import GammaRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "daM5Y0WY3N7g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bglok6-BCHZ"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUQXZq64MbDI"
      },
      "source": [
        "dat_mpg = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_III_7_auto-mpg.csv')\n",
        "dat_mpg.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npw581FNMbDJ"
      },
      "source": [
        "dat_mpg.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data explanation:\n",
        "* mpg - miles per gallon\n",
        "* horsepower - Engine horsepower\n",
        "* acceleration - Time to accelerate from 0 to 60 mph (sec.)\n",
        "* origin - Origin of car (A. American, B. European, C. Japanese)"
      ],
      "metadata": {
        "id": "5HPo1pDIpSbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run an OLS Regression and Analyze\n",
        "\n",
        "Typically, we would do some data exploration. However, instead we are going to run a OLS linear regression as a baseline. We will then check model validity, and plot some relationships that may be tricky. This is to showcase that running a model \"blind\" can be problematic."
      ],
      "metadata": {
        "id": "DABYJ1773LnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = dat_mpg['mpg']\n",
        "X = dat_mpg.drop(columns=['mpg','origin'])\n",
        "X = pd.concat([X,pd.get_dummies(dat_mpg['origin'], drop_first=True)], axis =1)\n",
        "X = sm.add_constant(X) # Add a constant term as the default model doesn't include one\n",
        "model_ols = sm.OLS(y, X.astype(float)).fit()\n",
        "model_ols.summary()"
      ],
      "metadata": {
        "id": "V5ATcLs2327X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we see that some of the \"fit metrics\" are indicating a poor firt. The skew is positive (i.e., the distibution seems skewed) and the kurtosis is larger than three. Furthermore, the Jaque-Bera test seeems to reject a normal assumption (although the sample size is somewhat small for the JB test).\n",
        "\n",
        "Let's look at the residual plots for model validation:"
      ],
      "metadata": {
        "id": "QjrePaoEP5CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "sns.histplot(model_ols.resid, bins=10, ax=axes[0], kde=True)\n",
        "axes[0].set_xlabel('Residuals')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Histogram of Residuals')\n",
        "sm.qqplot(model_ols.resid, line='s', ax=axes[1])\n",
        "axes[1].set_title('Normal Q-Q Plot of Residuals')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PqI6Wtx06_MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice the skewness and the tails of the distributions do not seem to be approproately captured by the normal distribution.\n",
        "\n",
        "Let's also plot the errors as a function of the feature variables horsepower and acceleration--recall that the assumption of OLS linear regression is an iid. error distribution with a constant variance:"
      ],
      "metadata": {
        "id": "00ThVNCQ5q6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "axes[0].scatter(dat_mpg['horsepower'], model_ols.resid)\n",
        "axes[0].set_xlabel('Horsepower')\n",
        "axes[0].set_ylabel('Residuals')\n",
        "axes[0].axhline(0, color='red')\n",
        "\n",
        "axes[1].scatter(dat_mpg['acceleration'], model_ols.resid)\n",
        "axes[1].set_xlabel('Acceleration')\n",
        "axes[1].set_ylabel('Residuals')\n",
        "axes[1].axhline(0, color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pfzuprspghJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that the assumption of a constant variance around the zero does not seem like an appropriate assumption.\n",
        "\n",
        "Let's also plot the target variable as a function of the features horsepower and acceleration -- recall that the OLS linear regression assumption is that $y$ depends linearly on $x_k$:"
      ],
      "metadata": {
        "id": "F0bvUyC0ghgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot mpg vs horsepower with linear trendline\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.regplot(x=dat_mpg['horsepower'], y=dat_mpg['mpg'], line_kws={'color': 'red'})\n",
        "# Add exponential trendline\n",
        "sns.regplot(x=dat_mpg['horsepower'], y=dat_mpg['mpg'], order=2, line_kws={'color': 'blue'})\n",
        "\n",
        "# Plot mpg vs acceleration with linear trendline\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.regplot(x=dat_mpg['acceleration'], y=dat_mpg['mpg'], line_kws={'color': 'red'})\n",
        "# Add exponential trendline\n",
        "sns.regplot(x=dat_mpg['acceleration'], y=dat_mpg['mpg'], order=2, line_kws={'color': 'blue'})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N-j9nxJF5wHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see that the data are not well matched by the red linear regression line, further indicating potential issues with the OLS linear regression assumption.\n",
        "\n",
        "Finally, let's compare the outcomes with the predictions:"
      ],
      "metadata": {
        "id": "ILHdIFtXSm-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(dat_mpg['mpg'], model_ols.predict())\n",
        "plt.plot([min(dat_mpg['mpg']), max(dat_mpg['mpg'])], [min(dat_mpg['mpg']), max(dat_mpg['mpg'])], 'r--')\n",
        "plt.xlabel('Actual mpg')\n",
        "plt.ylabel('Predicted mpg')\n",
        "plt.title('Actual vs. Predicted mpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zGanAo2Zm3bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it doesn't look very satisfactory overall..."
      ],
      "metadata": {
        "id": "6qIZXX7xnViG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gamma Regression\n",
        "\n",
        "So let's try a Gamma regression with a log-link function.\n",
        "\n",
        "### Why and what does that mean?\n",
        "\n",
        "Formally, a Gamma regression with log-link assumes:\n",
        "\n",
        "$$\n",
        "y_i | x_i ~ Gamma(\\mu_i | \\varphi), \\text{ where } \\log\\{\\mu_i\\} = \\beta_0 + \\beta_1\\,x_{i1}+\\ldots+\\beta_p\\,x_{ip} = x_i \\, \\beta\n",
        "$$\n",
        "\n",
        "Note that this means for the expected value and the variance of a random sample $Y_i$ with features $x_i$\n",
        "\n",
        "\\begin{eqnarray*}\n",
        "E[Y_i | x_i] &=& \\mu_i = e^{ \\beta_0 + \\beta_1\\,x_{i1}+\\ldots+\\beta_p\\,x_{ip}} = e^{x_i \\, \\beta}\\\\\n",
        "Var[Y_i | x_i ] &=& \\mu_i^2 \\, \\varphi.\n",
        "\\end{eqnarray*}\n",
        "\n",
        "Hence, the assumptions differ from OLS linear regression in several ways:\n",
        "\n",
        "* We assume $Y$ is Gamma distributed. The Gamma distribution is positive, skewed, and has heavier tails than the Normal distribution.\n",
        "* We assume that $Y$ depends on $x$ in an exponential fashion, as the equation for the expected value makes clear. We included an exponential trend\n",
        "\n",
        "So let's run the Gamma regression using the statsmodel GLM routines (note that we need to defined the log function, because the standard link function for Gamma regression is different):"
      ],
      "metadata": {
        "id": "vxdFr4X8dqds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_g = sm.genmod.families.links.log\n",
        "model_gamma = sm.GLM(y, X.astype(float), family=sm.families.Gamma(link_g())).fit()\n",
        "model_gamma.summary()"
      ],
      "metadata": {
        "id": "2Ce5o2vodp2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### So, is it better?\n",
        "\n",
        "The short answer is yes. The pseudo-R-squared (determined as the correlation of predictions and observations, squared, as we discussed before) with 93% is substantively higher than the OLS R-squared of 68%. The Pearson chi2 goodness of fit metric with 11.9 isn't gigantic (though it's also not very small, which would indicated a very good fit).\n",
        "\n",
        "Let's compare predictions and outcomes here:"
      ],
      "metadata": {
        "id": "hCytM7XOlgf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(dat_mpg['mpg'], model_gamma.predict())\n",
        "plt.plot([min(dat_mpg['mpg']), max(dat_mpg['mpg'])], [min(dat_mpg['mpg']), max(dat_mpg['mpg'])], 'r--')\n",
        "plt.xlabel('Actual mpg')\n",
        "plt.ylabel('Predicted mpg')\n",
        "plt.title('Actual vs. Predicted mpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dYGLHn9Hns5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the fit seems better, although we still seem to underpredict some for very high mpg values.\n",
        "\n",
        "Let's compare the linear regression and the gamma regression predictions:"
      ],
      "metadata": {
        "id": "e9SrNDJCoiMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(model_ols.predict(), model_gamma.predict())\n",
        "plt.xlabel('Linear Regression Predictions')\n",
        "plt.ylabel('Gamma Regression Predictions')\n",
        "plt.plot([min(model_ols.predict()), max(model_ols.predict())], [min(model_ols.predict()), max(model_ols.predict())], 'r--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wUrbcw_PoGea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do we interpret the Gamma regression?"
      ],
      "metadata": {
        "id": "kHSZIuAco6cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The directional interpretation is analogous: A positive coefficient is indicative of an increasing relationship, and vice versa for a negative relationship. Furthermore, we intepret the standard errors, p-values, and confidence intervals as before.\n",
        "\n",
        "However, for the magnitude we have to incorporate the link-function. The way to think about it with the relationship about the expected value above is:\n",
        "$$\n",
        "E[\\text{mpg} | \\text{acceleration} +1] = E[\\text{mpg} | \\text{acceleration}] \\times e^{-0.0235} =\n",
        "$$"
      ],
      "metadata": {
        "id": "bNCsD3z5BrAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(-0.0235)"
      ],
      "metadata": {
        "id": "NqIK6SG5rRV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so, it is reduced by a *factor* of about 2.3% per acceleration unit.\n",
        "\n",
        "Similarly, going from region A to region B will change mpg by:\n",
        "$$\n",
        "E[\\text{mpg} | \\text{region B}] = E[\\text{mpg} | \\text{region A} \\times e^{-0.0235} =\n",
        "$$"
      ],
      "metadata": {
        "id": "7LqVaFkwrWcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(0.0893)"
      ],
      "metadata": {
        "id": "xjexU2Xtr8RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So mpg increases by roughly 9.3%."
      ],
      "metadata": {
        "id": "E0uzklm4sCVc"
      }
    }
  ]
}