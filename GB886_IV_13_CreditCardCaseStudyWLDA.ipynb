{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB886_IV_13_CreditCardCaseStudyWLDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credit Card Default Case Study\n",
        "\n"
      ],
      "metadata": {
        "id": "bFNuMreLe4xh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "In this tutorial, we will use logistic regression to predict credit card default scores.\n",
        "\n",
        "We rely on the dataset `pa_data_UCI_Credit_Card.csv` from the UCI Machine Learning Repository (Lichman, M., 2013. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).  This datasets provides credit card defaults for customers in Taiwan.  We are given some demographic information ($X_1$-$X_5$), the previous history of payments ($X_6$-$X_{11}$), the amount of previous bills ($X_{12}$-$X_{17}$), and amounts of previous payments ($X_{18}$-$X_{23}$).  Finally, variable 24 is our target, whetyher there was a default in the next months.\n",
        "\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset"
      ],
      "metadata": {
        "id": "okvP8ddqfGwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QK-AnxqWTB"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzW2COB-qWTC"
      },
      "source": [
        "mydata = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_IV_9_UCI_Credit_Card.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration and Preparation"
      ],
      "metadata": {
        "id": "Y4q8flL3fLw8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xrrgfqWTC"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some aggregate statistics."
      ],
      "metadata": {
        "id": "bdiFv-p4fQ1j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYoH_ngqWTD"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, a number of the variables are included numerically but really they have factor character, particularly Gender (1 = male; 2 = female), Education (1 = graduate school; 2 = university; 3 = high school; 4 = others), Marital status (1 = married; 2 = single; 3 = others), and default payment. Let's store them as factors.  We will do the same for history of past payment ($X_6$-$X_{11}$), although they really have ordinal character."
      ],
      "metadata": {
        "id": "651lfbJMgE04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factor = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'default.payment.next.month']"
      ],
      "metadata": {
        "id": "ZAwjAtXxgJ9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, a number of the levels occur very sparsely: there are 11 levels for all the `PAY` variables, but only the first six seem to be frequent.  So let's collapse levels 7 through 11 to one:"
      ],
      "metadata": {
        "id": "23vcLxDYgRcb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5L46eWqWTD"
      },
      "source": [
        "mydata['PAY_0'][mydata['PAY_0']>4] = 4\n",
        "mydata['PAY_2'][mydata['PAY_2']>4] = 4\n",
        "mydata['PAY_3'][mydata['PAY_3']>4] = 4\n",
        "mydata['PAY_4'][mydata['PAY_4']>4] = 4\n",
        "mydata['PAY_5'][mydata['PAY_5']>4] = 4\n",
        "mydata['PAY_6'][mydata['PAY_6']>4] = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, generate dummies:"
      ],
      "metadata": {
        "id": "Q4xm1bOegjmw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rfdm9IdqWTE"
      },
      "source": [
        "mydata_numcols = mydata.drop(columns = factor)\n",
        "mydata_faccols = mydata[factor].astype('category')\n",
        "dummies = pd.get_dummies(mydata_faccols, drop_first=True)\n",
        "mydata = pd.concat([mydata_numcols, dummies], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And Let's relabel the long name of the dependent variable:"
      ],
      "metadata": {
        "id": "tSOZn3i-gtKf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCaaHETUqWTF"
      },
      "source": [
        "mydata = mydata.rename(columns={\"default.payment.next.month_1\": \"default\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "fwfWHqovgynk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UfQEQiEqWTF"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zibCpbmrqWTG"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check a correlation plot to make sure none of the variables is extremely correlated:"
      ],
      "metadata": {
        "id": "HdHdC-Kyg5A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.triu(np.ones_like(mydata.corr(), dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(mydata.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "metadata": {
        "id": "xJPkW_qwwKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like the bill amounts are highly correlated.  Let's just keep the most recent one and then the average of all of them:\n"
      ],
      "metadata": {
        "id": "ihKWiDkphM_3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIXJYlyqWTH"
      },
      "source": [
        "mydata.insert(17, \"BILL_AVG\", (mydata['BILL_AMT1']+mydata['BILL_AMT2']+mydata['BILL_AMT3']+mydata['BILL_AMT4']+mydata['BILL_AMT5']+mydata['BILL_AMT6'])/6, True)\n",
        "mydata = mydata.rename(columns={\"BILL_AMT1\": \"BILL_REC\"})\n",
        "del mydata['BILL_AMT2']\n",
        "del mydata['BILL_AMT3']\n",
        "del mydata['BILL_AMT4']\n",
        "del mydata['BILL_AMT5']\n",
        "del mydata['BILL_AMT6']\n",
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the dataset so that we can use it in coming tutorials without having to go through this procedure again:"
      ],
      "metadata": {
        "id": "IdJvKO_FhSkU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIpqkhsqWTH"
      },
      "source": [
        "mydata.to_csv('GB886_IV_9_UCI_Credit_Card_prepped.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling: Logistic Regression"
      ],
      "metadata": {
        "id": "Xwm41PjahXf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define\n"
      ],
      "metadata": {
        "id": "e80dFeKijLfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8HBxrBqWTI"
      },
      "source": [
        "y = mydata['default']\n",
        "X = mydata.drop(columns = ['default'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_mod = sm.Logit(y, sm.add_constant(X).astype(float))\n",
        "logistic_mod = logistic_mod.fit()\n",
        "print(logistic_mod.summary())"
      ],
      "metadata": {
        "id": "yJRgok_IQFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we notice the limit balance and the pay amounts have a negative association with default, whereas the bill average and the bill average have a positive association with default. Several of the demographic variables seem to matter, too!"
      ],
      "metadata": {
        "id": "m--mKwsWWfXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(I also ran this regression using sklearn, but the coefficients were quite different. There are a few reasons that I explored, but in the end the results still did not align. Given that there is a numerical procedure involved in solving the model, sometimes small inconsistencies can lead to substantial differences in coefficients.)"
      ],
      "metadata": {
        "id": "l1BP7jjcjPzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check predictions:"
      ],
      "metadata": {
        "id": "PxNquZnljYn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_x = logistic_mod.predict()\n",
        "y_hat = (p_x > 0.5)"
      ],
      "metadata": {
        "id": "b-CbR92JxFfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the **confusion table**:"
      ],
      "metadata": {
        "id": "ESvCIt_xyseC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat = pd.crosstab(y, y_hat, rownames=['Actual Defaults'], colnames=['Predicted Defualts'])\n",
        "# Add row and column sums\n",
        "conf_mat.loc['Column_Total']= conf_mat.sum(numeric_only=True, axis=0)\n",
        "conf_mat.loc[:,'Row_Total'] = conf_mat.sum(numeric_only=True, axis=1)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "IxKjglK_xxPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's calculate some resulting metrics:"
      ],
      "metadata": {
        "id": "o4rroO9Vy1kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TPR = 2372 / 6636 # True-Positive Rate\n",
        "TNR = 22272 / 23364 # True-Negative Rate\n",
        "MCR = (1092+4264)/30000 # Miss Classification Rate\n",
        "print('TPR =', TPR)\n",
        "print('TNR =', TNR)\n",
        "print('MCR =', MCR)"
      ],
      "metadata": {
        "id": "5J-c4YuiyWk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So we are missing a few, yet the misclassification rate seems reasonable.\n",
        "\n",
        "Let's consider the **ROC curve**:"
      ],
      "metadata": {
        "id": "4pC5K5zNjbXe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIY4iAiwqWTL"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve(y, p_x)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the AUC is 77%---which is ok but it is clear that in order to get a high true positive rate (e.g., 80%+), we need to have to accept a high FPR (e.g., of more than 40%+). Default prediction is not a simple problem!"
      ],
      "metadata": {
        "id": "dMVhY4OozL_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling: LDA"
      ],
      "metadata": {
        "id": "fCXohrkEz2GM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check the LDA Classifier, going through similar steps:"
      ],
      "metadata": {
        "id": "HzaeVCisdhPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "w4Be5PJ20FO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.astype('int')"
      ],
      "metadata": {
        "id": "njD1m7fT0fNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LinearDiscriminantAnalysis()\n",
        "lda_model.fit(X,y.astype('int'))\n",
        "lda_pred = lda_model.predict(X)"
      ],
      "metadata": {
        "id": "gp4AMjYidhnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y,lda_pred)"
      ],
      "metadata": {
        "id": "ZF-z6icU0uoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's look at the ROC curve and AUC:"
      ],
      "metadata": {
        "id": "yGyK3XSPdpfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_pred_proba = lda_model.predict_proba(X)\n",
        "fpr, tpr, threshold = roc_curve(y, lda_pred_proba[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qExjbVDlduAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So very similar!"
      ],
      "metadata": {
        "id": "NjxjnKy51E3s"
      }
    }
  ]
}