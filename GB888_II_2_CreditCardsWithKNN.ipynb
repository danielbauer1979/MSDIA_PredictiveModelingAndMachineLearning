{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_II_2_CreditCardsWithKNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credit Card Defaults with knn"
      ],
      "metadata": {
        "id": "bFNuMreLe4xh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "In this tutorial, we will go back to the credit card default scores -- but this time we will us knn instead of logistic regression.\n",
        "\n",
        "## KNN Background\n",
        "\n",
        "Another so-called *algorithmic* learners use different structural assumptions. For instance, we illustrate a **k-nearest neighbor (knn)** approach, where the predicted class at a point $x_0$ is chosen based on the $k$ points that are closest:\n",
        "$$\n",
        "y(x_0) = \\max_j\\left\\{\\frac{1}{K} \\sum_{i \\in N_K(x_0)} 1_{\\{y_i=j\\}}\\right\\},\n",
        "$$\n",
        "where $N_k(x_0)$ denotes the index set of the $K$ points in the training sample that are closest to the point $x_0$ (usually in the sense of Euclidean distance).  This is very differnt than what we have seen before in that we don't have an underlying \"probabilistic\" approach.\n",
        "\n",
        "As a reminder, the dataset provides credit card defaults for customers in Taiwan.  We are given some demographic information and information whether there was a default in the next months. We had prepared the data in GB 886, see [here](https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB886_IV_12_CreditCardCaseStudy.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Credit Card defaults\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ],
      "metadata": {
        "id": "4UM75kOvt0Mh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's load the prepared the dataset (see the steps from before):"
      ],
      "metadata": {
        "id": "okvP8ddqfGwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QK-AnxqWTB"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzW2COB-qWTC"
      },
      "source": [
        "mydata = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_IV_12_UCI_Credit_Card_prepped.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydata.head()"
      ],
      "metadata": {
        "id": "Z1AZPC9nvABs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also will split our dataset, to evaluate how knn prepares to conventional algorithms:"
      ],
      "metadata": {
        "id": "11a5Er2zvG_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Train, Test = train_test_split(mydata, test_size=0.25)\n",
        "Train_y = Train['default']\n",
        "Train_X = Train.drop(columns = ['default'])\n",
        "Test_y = Test['default']\n",
        "Test_X = Test.drop(columns = ['default'])"
      ],
      "metadata": {
        "id": "DHHum-bHvGPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling: Baseline Logistic Regression"
      ],
      "metadata": {
        "id": "Xwm41PjahXf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's again run our baseline logistic regression model:\n"
      ],
      "metadata": {
        "id": "e80dFeKijLfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_mod = sm.Logit(Train_y, sm.add_constant(Train_X).astype(float))\n",
        "logistic_mod = logistic_mod.fit(maxiter = 10000)\n",
        "print(logistic_mod.summary())"
      ],
      "metadata": {
        "id": "yJRgok_IQFBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first check the predictions in the training set:"
      ],
      "metadata": {
        "id": "r3EWee-nyJ84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_pred = logistic_mod.predict()\n",
        "logistic_pred_lab = logistic_pred > 0.5\n",
        "conf_matrix = confusion_matrix(Train_y, logistic_pred_lab)\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "vxBkpCHMyJR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And some of the prediction rates are:"
      ],
      "metadata": {
        "id": "LPvERiObyU8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the misclassification rate\n",
        "MCR = 1 - (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"Misclassification Rate: {MCR}\")\n",
        "\n",
        "# Calculate the False Positive Rate (FPR)\n",
        "FPR = FP / (FP + TN)\n",
        "print(f\"False Positive Rate (FPR): {FPR}\")\n",
        "\n",
        "# Calculate the False Negative Rate (FNR)\n",
        "FNR = FN / (FN + TP)\n",
        "print(f\"False Negative Rate (FNR): {FNR}\")"
      ],
      "metadata": {
        "id": "rmwvRWOFyYOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's check the predictions via the confusion matrix in the Test set:"
      ],
      "metadata": {
        "id": "PxNquZnljYn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_pred = logistic_mod.predict(sm.add_constant(Test_X).astype(float))\n",
        "logistic_pred_lab = logistic_pred > 0.5\n",
        "conf_matrix = confusion_matrix(Test_y, logistic_pred_lab)\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "3SzB1vTqv_AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And some of the prediction rates are:"
      ],
      "metadata": {
        "id": "KqHcoKsdyFij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the misclassification rate\n",
        "MCR = 1 - (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"Misclassification Rate: {MCR}\")\n",
        "\n",
        "# Calculate the False Positive Rate (FPR)\n",
        "FPR = FP / (FP + TN)\n",
        "print(f\"False Positive Rate (FPR): {FPR}\")\n",
        "\n",
        "# Calculate the False Negative Rate (FNR)\n",
        "FNR = FN / (FN + TP)\n",
        "print(f\"False Negative Rate (FNR): {FNR}\")"
      ],
      "metadata": {
        "id": "TlxnWAUzzJc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling: knn"
      ],
      "metadata": {
        "id": "J5tWsDp_xMql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run the knn algorithm. We first also consider the test set:"
      ],
      "metadata": {
        "id": "J3FEITl8xhcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_model.fit(Train_X, Train_y)\n",
        "Train_y_knn = knn_model.predict(Train_X)\n",
        "Test_y_knn = knn_model.predict(Test_X)"
      ],
      "metadata": {
        "id": "itV3wGFLw0b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the performance in the training set:"
      ],
      "metadata": {
        "id": "3Pq8zJDKzzlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(Train_y, Train_y_knn)\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "-Y8KFSiWzsjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the misclassification rate\n",
        "MCR = 1 - (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"Misclassification Rate: {MCR}\")\n",
        "\n",
        "# Calculate the False Positive Rate (FPR)\n",
        "FPR = FP / (FP + TN)\n",
        "print(f\"False Positive Rate (FPR): {FPR}\")\n",
        "\n",
        "# Calculate the False Negative Rate (FNR)\n",
        "FNR = FN / (FN + TP)\n",
        "print(f\"False Negative Rate (FNR): {FNR}\")"
      ],
      "metadata": {
        "id": "pRLo4lCYz6Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we're beating the logistic regression results---in the Training set, though.\n",
        "\n",
        "Let's look at the Test set:"
      ],
      "metadata": {
        "id": "-KmjT5Zzz9nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(Test_y, Test_y_knn)\n",
        "TN, FP, FN, TP = conf_matrix.ravel()\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "DyasKt010H69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the misclassification rate\n",
        "MCR = 1 - (TP + TN) / (TP + TN + FP + FN)\n",
        "print(f\"Misclassification Rate: {MCR}\")\n",
        "\n",
        "# Calculate the False Positive Rate (FPR)\n",
        "FPR = FP / (FP + TN)\n",
        "print(f\"False Positive Rate (FPR): {FPR}\")\n",
        "\n",
        "# Calculate the False Negative Rate (FNR)\n",
        "FNR = FN / (FN + TP)\n",
        "print(f\"False Negative Rate (FNR): {FNR}\")"
      ],
      "metadata": {
        "id": "Tm0YLOFR0Oo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, here we are performing quite a bit worse. Maybe we didn't choose the right k? Let's check the misclassification rate in the test set for different choices of k:"
      ],
      "metadata": {
        "id": "n5MPbQAW0SKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [2, 5, 10, 15, 20]\n",
        "misclass_rates = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn_model.fit(Train_X, Train_y)\n",
        "    Test_y_knn = knn_model.predict(Test_X)\n",
        "    conf_matrix = confusion_matrix(Test_y, Test_y_knn)\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    MCR = 1 - (TP + TN) / (TP + TN + FP + FN)\n",
        "    misclass_rates.append(MCR)\n",
        "\n",
        "plt.plot(k_values, misclass_rates, marker='o')\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Misclassification Rate')\n",
        "plt.title('Misclassification Rate vs. k for KNN')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lnbrlq620enY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, it appears that we don't get close to the misclassification rate for any of the choices. Hence, knn doesn't seem like a great choice in this setting."
      ],
      "metadata": {
        "id": "B6EyHbXd1ECs"
      }
    }
  ]
}