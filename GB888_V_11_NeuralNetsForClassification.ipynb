{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_V_11_NeuralNetsForClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks For Regression\n",
        "\n",
        "\n",
        "In this tutorial, we then use neural networks in our case study example for the Caravan Insurance purchases, analyzing whether they can improve on the learners considered so far.\n",
        "\n",
        "As usually, let's start with loading the relevant libaries."
      ],
      "metadata": {
        "id": "_Dgf3D6YiA0C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Case Study: Caravan Insurance Purchases\n",
        "\n",
        "Let's go back to the `Caravan` insurance data:"
      ],
      "metadata": {
        "id": "zNnvu0I48wYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "metadata": {
        "id": "Gb1EdVnT8xy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Caravan = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB888_III_7_CaravanData.csv', index_col=0)"
      ],
      "metadata": {
        "id": "zhmtJl7D82OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split the dataset, using the same approach as we did in the previous module:"
      ],
      "metadata": {
        "id": "v2IifQXX84Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Caravan.Purchase = (Caravan.Purchase=='Yes')\n",
        "train, test = train_test_split(Caravan, test_size=0.25, random_state=1)\n",
        "\n",
        "X = train.drop(['Purchase'], axis=1)\n",
        "y = train['Purchase']\n",
        "Xtest = test.drop(['Purchase'], axis=1)\n",
        "ytest = test['Purchase']"
      ],
      "metadata": {
        "id": "YmQbQjam9BWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recall, we previously considered a logistic regression model. It produced an AUC of .71 but did not determine a single true positive for a 50% cutoff. We also considered a pruned tree which produced an AUC of .72, which was able to determine a few true positives!"
      ],
      "metadata": {
        "id": "j3cAUCS6tAhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Random Forest"
      ],
      "metadata": {
        "id": "II5Xb1RT-a-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with a random forest (with default parameters, so no tuning for now):"
      ],
      "metadata": {
        "id": "SxOEFk5Q_tGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(random_state=1)\n",
        "rf.fit(X, y)"
      ],
      "metadata": {
        "id": "aOXgXBgU-dd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To appraise what features matter, let's consider feature importance scores:"
      ],
      "metadata": {
        "id": "OP9MaM0h_wFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Importance_ = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=X.columns)\n",
        "Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]\n",
        "Importance.plot(kind='barh', color='b', ).invert_yaxis()\n",
        "plt.xlabel('Variable Importance')\n",
        "plt.gca().legend_ = None"
      ],
      "metadata": {
        "id": "BN8FUuQ1-hay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's look at the predictions:"
      ],
      "metadata": {
        "id": "f6t_rsaH_zsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_rf = rf.predict_proba(Xtest)\n",
        "pred_rf = pred_rf[:,1]"
      ],
      "metadata": {
        "id": "1Ff_lxHt-un9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And ROC curve/AUC:"
      ],
      "metadata": {
        "id": "y01n0qIH_5mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(ytest, pred_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UDlvRmp7-yaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "y_pred_class = (pred_rf > threshold).astype(int)\n",
        "\n",
        "conf_matrix = confusion_matrix(ytest, y_pred_class)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "CoINzRaBvsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not quite the same performance as the pruned (!) tree or the logistic regression model. Let's try a second random forest with alternate parameters, a few more trees and 45 features sampled per tree:"
      ],
      "metadata": {
        "id": "Ri3NA_kp_7fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=400, max_features=45, random_state=1)\n",
        "rf.fit(X, y)\n",
        "pred_rf = rf.predict_proba(Xtest)\n",
        "pred_rf = pred_rf[:,1]"
      ],
      "metadata": {
        "id": "0hRXDgdSwIdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the AUC:"
      ],
      "metadata": {
        "id": "oYO1SqqOyXJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(ytest, pred_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J9dI8vCaxUGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it gets a little better but not much..."
      ],
      "metadata": {
        "id": "oXCC2F4Jybsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boosting"
      ],
      "metadata": {
        "id": "lvzknaCb96qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a gradient boosting model, again with the standard parameters:"
      ],
      "metadata": {
        "id": "xlQu_oRg_V6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boost = GradientBoostingClassifier(random_state=1)\n",
        "boost.fit(X, y)"
      ],
      "metadata": {
        "id": "_PX2C-Kw-Hmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To appraise what features matter, let's consider feature importance scores:"
      ],
      "metadata": {
        "id": "w9UBr7X8_YPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = boost.feature_importances_*100\n",
        "rel_imp = pd.Series(feature_importance, index=X.columns).sort_values(ascending=False, inplace=False)\n",
        "rel_imp = rel_imp[0:20]\n",
        "print(rel_imp)\n",
        "rel_imp.plot(kind='barh', color='b', ).invert_yaxis()\n",
        "plt.xlabel('Variable Importance')"
      ],
      "metadata": {
        "id": "jp4Bjexf-NRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here the important features are quite different.\n",
        "\n",
        "The predictions are:"
      ],
      "metadata": {
        "id": "EyovqtSR_hqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_boost = boost.predict_proba(Xtest)\n",
        "pred_boost = pred_boost[:,1]"
      ],
      "metadata": {
        "id": "VddIufzC-Voe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resulting in the following ROC curve and AUC:"
      ],
      "metadata": {
        "id": "kKsmeqdN_l-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(ytest, pred_boost)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lzc8IX1J-aKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the performance improves quite a bit, and we are also beating the pruned tree and the logistic regression model.\n",
        "\n"
      ],
      "metadata": {
        "id": "5nEEFBgyzD8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tune the parameters, let's implement a rough grid search. Let's define the parameter grid for the grid search"
      ],
      "metadata": {
        "id": "hwteRCzvX0aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_estimators_grid = [100, 500, 1000]\n",
        "learning_rate_grid = [0.001, 0.01, 0.1]"
      ],
      "metadata": {
        "id": "vSxAub7sX-BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's cycle through the choices to find the best combination:"
      ],
      "metadata": {
        "id": "uL0OFREUX_Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_auc = 0\n",
        "best_params = {}\n",
        "\n",
        "for n_estimators in n_estimators_grid:\n",
        "  for learning_rate in learning_rate_grid:\n",
        "    # Train the GradientBoostingClassifier with the current parameters\n",
        "    boost = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=1)\n",
        "    boost.fit(X, y)\n",
        "\n",
        "    # Predict probabilities for the test set\n",
        "    pred_boost = boost.predict_proba(Xtest)[:, 1]\n",
        "\n",
        "    # Calculate the AUC score\n",
        "    auc_score = roc_auc_score(ytest, pred_boost)\n",
        "\n",
        "    # Update the best AUC and parameters if the current AUC is better\n",
        "    if auc_score > best_auc:\n",
        "        best_auc = auc_score\n",
        "        best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n",
        "\n",
        "print(\"Best AUC:\", best_auc)\n",
        "print(\"Best parameters:\", best_params)"
      ],
      "metadata": {
        "id": "UloWqCTuXWDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So a little better. Let's look at the confision matrix:"
      ],
      "metadata": {
        "id": "TUxejbMVZAeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boost = GradientBoostingClassifier(n_estimators=best_params['n_estimators'], learning_rate=best_params['learning_rate'], random_state=1)\n",
        "boost.fit(X, y)\n",
        "pred_boost = boost.predict_proba(Xtest)\n",
        "pred_boost = pred_boost[:,1]\n",
        "threshold = 0.15 # 0.5 gives only zeros...\n",
        "y_pred_class = (pred_boost > threshold).astype(int)\n",
        "conf_matrix = confusion_matrix(ytest, y_pred_class)\n",
        "print(\"Confusion Matrix:\")\n",
        "conf_matrix"
      ],
      "metadata": {
        "id": "1EB53EJTZHcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADA Boosting\n",
        "\n",
        "Let's finally look at the ADA boost"
      ],
      "metadata": {
        "id": "4tfsqJ250nC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada = AdaBoostClassifier(random_state=1)  # Adjust n_estimators as needed\n",
        "ada.fit(X, y)"
      ],
      "metadata": {
        "id": "Ak1qy0znUeoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With predictions:"
      ],
      "metadata": {
        "id": "_Bf2ozIx1zHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_ada = ada.predict_proba(Xtest)[:, 1]"
      ],
      "metadata": {
        "id": "-xQEnM3e1XvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's generate an ROC curve:"
      ],
      "metadata": {
        "id": "Gdbjbqtm14PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(ytest, pred_ada)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic (AdaBoost)')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CxCkT4Jd13Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So again a pretty decent performance with an AUC of .75."
      ],
      "metadata": {
        "id": "otjw2MJN1_iJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also go with a lower learning rate but more trees:"
      ],
      "metadata": {
        "id": "fcqFGxbW2IBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada = AdaBoostClassifier(n_estimators=1000,learning_rate=0.04,random_state=1)  # Adjust n_estimators as needed\n",
        "ada.fit(X, y)\n",
        "\n",
        "# Predictions\n",
        "pred_ada = ada.predict_proba(Xtest)[:, 1]  # Probability of positive class\n",
        "\n",
        "# ROC curve and AUC\n",
        "fpr, tpr, threshold = roc_curve(ytest, pred_ada)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic (AdaBoost)')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HbSLFa6S1aoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not really an improvement here."
      ],
      "metadata": {
        "id": "DD5QBNMC2W4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, we could tune by more systematically going through the hyper-parameters, but we observe that also the standard parameters produce decent models."
      ],
      "metadata": {
        "id": "a-bMgN4O2bTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural Networks\n",
        "\n",
        "Let's spin up a simple neural network and see if we can improve. We scale the feature matrices:"
      ],
      "metadata": {
        "id": "iBUabB0McrzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Xtest_scaled = scaler.transform(Xtest)"
      ],
      "metadata": {
        "id": "wBiag9RddOVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the shape..."
      ],
      "metadata": {
        "id": "uKbg_Gu1eyVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_scaled.shape"
      ],
      "metadata": {
        "id": "8jNFTr2hd1b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...so, our input layer has 85 neurons. Let's go with a neural net with three layers using relu and sigmoid functions as activation and 6, 5, and 4 layers. For the output layer, we want a sigmoid function because we need something between zero and one:"
      ],
      "metadata": {
        "id": "HwgPZHoTe0jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(85,))\n",
        "x = layers.Dense(6, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "x = layers.Dense(5, activation=\"relu\", name=\"dense_2\")(x)\n",
        "x = layers.Dense(4, activation=\"sigmoid\", name=\"dense_3\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "credit_nnet = keras.Model(inputs=inputs, outputs=outputs)\n",
        "credit_nnet.compile(\n",
        "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "  # Loss function to minimize\n",
        "  loss='binary_crossentropy',\n",
        "  # List of metrics to monitor\n",
        "  metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "Bqn0vTdZcq-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are using accuracy as a metric to report, but we are minimizing cross-entropy.\n",
        "\n",
        "Let's fit using 50 eopochs:"
      ],
      "metadata": {
        "id": "EvnmOsiLfIUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = credit_nnet.fit(\n",
        "  X_scaled,\n",
        "  y,\n",
        "  batch_size=45,\n",
        "  epochs=50\n",
        ")"
      ],
      "metadata": {
        "id": "bwCdz85neEpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is unclear whether our net is well-fit at this stage, we should monitor in a validation sample. But for now let's run with this. Let's check the performance in the test set:"
      ],
      "metadata": {
        "id": "JvysVLElfYK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mypreds = credit_nnet.predict(Xtest_scaled)\n",
        "mypreds_bin = mypreds > 0.5"
      ],
      "metadata": {
        "id": "srF-9ppmeScK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = pd.DataFrame({'True':ytest,'pred':mypreds_bin.T[0]})\n",
        "table.groupby(['True','pred']).size().unstack('True')"
      ],
      "metadata": {
        "id": "JjaYD4l_eXgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(ytest, mypreds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eR1xze6ReeHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we're quite far away from all the other learners... Surely we can do better by trying harder, but that's part of the point here: Random forests and gradient boosting models are very good off-the-shelf learners. Neural nets require more compute time and more experimentation. Whether that effort is warranted clearly depends on the application."
      ],
      "metadata": {
        "id": "5FiYJ4G8fn9O"
      }
    }
  ]
}