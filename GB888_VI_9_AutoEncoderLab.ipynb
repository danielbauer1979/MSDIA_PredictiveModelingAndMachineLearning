{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+0t4YrLOKrcEd9bTQ6myf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_VI_9_AutoEncoderLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab: A Simple Image Autencoder\n",
        "\n",
        "In this lab, we will demonstrate a simple auto-encoder in the context of the Fashion MNIST dataset. We opt for simplicity and use single-layer encoders and decoders. Obviouly, we could enhance performance by having deep, convolutional layers in the encoding and decoding steps. For a more detailed example in the context of the MNIST digit dataset, see this [keras blog article](https://blog.keras.io/building-autoencoders-in-keras.html).\n",
        "\n",
        "## Import Packages and Data\n",
        "\n",
        "As usually, we import keras functionality:"
      ],
      "metadata": {
        "id": "3a3jpa3eeqRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6rCM6FQbk7C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's import the Fashion MNIST data:"
      ],
      "metadata": {
        "id": "8o55kUlRf6Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "ICWqfTYEbxUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's normalize and flatten the images (as we use a conventional neural net):"
      ],
      "metadata": {
        "id": "Oa45u9t7f_Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "cKzDBEhdbyYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple Autoencoder\n",
        "\n",
        "### Defining the autoencoder\n",
        "\n",
        "We will consider a simple autoencoder with 32 neurons in the middle. This is the size of our encoded representations! 32 floats means we have a compression of factor 24.5, since the input is 784 floats."
      ],
      "metadata": {
        "id": "KbDvpaylgLP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_dim = 32"
      ],
      "metadata": {
        "id": "Rn96FWIAe9YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our autoencoder consists of the Input, a single encoder layer using Relu-s, and single decoder layer going back to the input size:"
      ],
      "metadata": {
        "id": "O-GANEkYgema"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_img = keras.Input(shape=(784,))\n",
        "encoded = layers.Dense(encoding_dim, activation='relu')(input_img)\n",
        "decoded = layers.Dense(784, activation='sigmoid')(encoded)"
      ],
      "metadata": {
        "id": "pEiRC9ooe6zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our autoencoder combines encoder and decoder:"
      ],
      "metadata": {
        "id": "S-qC2ddOgwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = keras.Model(input_img, decoded)"
      ],
      "metadata": {
        "id": "glKhpHGCgvQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also separately defined the encoder: This model maps an input to its encoded representation."
      ],
      "metadata": {
        "id": "Suupnm_og3OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = keras.Model(input_img, encoded)"
      ],
      "metadata": {
        "id": "FpA4NlP7bnZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's define our decoder:"
      ],
      "metadata": {
        "id": "7RLbH2DOhEF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = keras.Input(shape=(encoding_dim,))\n",
        "decoder_layer = autoencoder.layers[-1]\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
      ],
      "metadata": {
        "id": "JHcaueZ9brjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training our autoencoder"
      ],
      "metadata": {
        "id": "ve71qYR-hZ8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use binary-crossentropy to assess the similarity between the pixels (recall these are between zero and one). We could also use a regression objective."
      ],
      "metadata": {
        "id": "bcSBhBFQhcjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "e4BcAXCbbus3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's train the model. Again, the idea is that x is our feature vector *and* our target!"
      ],
      "metadata": {
        "id": "gHj7lRsvhrM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "kAr_VgNob3Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating our Autoencoder\n",
        "\n",
        "Let's evaluate our autoencoder based on the test set."
      ],
      "metadata": {
        "id": "5xvNyAjwh5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_imgs = encoder.predict(x_test)\n",
        "decoded_imgs = decoder.predict(encoded_imgs)"
      ],
      "metadata": {
        "id": "J7-Plsjbb-4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do a visual inspection of a few images:"
      ],
      "metadata": {
        "id": "D1lklgxJiRmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XKSFI-Mlb_lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we note that by compressing, we definitely lose some detail. But the autoencoder does capture the basic shape!\n",
        "\n",
        "One interesting application is that we can evaluate similaries by considering the distance in compressed space. Let's check how similar the two pants are (third and fourth image):"
      ],
      "metadata": {
        "id": "QiWbCkD9imyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image_encoded = encoded_imgs[2]\n",
        "second_image_encoded = encoded_imgs[3]\n",
        "\n",
        "distance = np.linalg.norm(first_image_encoded - second_image_encoded)\n",
        "\n",
        "print(f\"The Euclidean distance between the images is: {distance}\")"
      ],
      "metadata": {
        "id": "e-jig8dtdIxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare this to the difference between the shoe and the sweater in the first two images:"
      ],
      "metadata": {
        "id": "tabI8SsojVsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image_encoded = encoded_imgs[0]\n",
        "second_image_encoded = encoded_imgs[1]\n",
        "\n",
        "distance = np.linalg.norm(first_image_encoded - second_image_encoded)\n",
        "\n",
        "print(f\"The Euclidean distance between the images is: {distance}\")"
      ],
      "metadata": {
        "id": "y4obiz1wjdgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Much larger.\n",
        "\n",
        "How about the sweater and the jacket:"
      ],
      "metadata": {
        "id": "4sYXq0N3jfpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_image_encoded = encoded_imgs[1]\n",
        "second_image_encoded = encoded_imgs[4]\n",
        "\n",
        "distance = np.linalg.norm(first_image_encoded - second_image_encoded)\n",
        "\n",
        "print(f\"The Euclidean distance between the images is: {distance}\")"
      ],
      "metadata": {
        "id": "p6rWvG0BjnlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the middle. You get the idea: The vector gives a numerical representation of the images in 32 dimensional space. And more similar images are closer than more different images!"
      ],
      "metadata": {
        "id": "5q_YDvWLjrwk"
      }
    }
  ]
}