{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_III_4_RegressionTreesIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Regression Trees\n",
        "\n",
        "In this tutorial, we will introduce regression trees.  We consider a basic simulated example with a single predictor that compares tress to (linear) regression.  In particular, we will demonstrate when trees do well and when the don't.\n",
        "\n",
        "As usually, let's start with loading the relevant libaries."
      ],
      "metadata": {
        "id": "_Dgf3D6YiA0C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import matplotlib.pyplot as plt\n",
        "import graphviz\n",
        "import pydot\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Trees\n",
        "\n",
        "## Review of Concepts and Maths\n",
        "\n",
        "### Trees\n",
        "\n",
        "As we discuss in the previous parts, trees successively split the domains of predictors into two areas (greater and smaller), so that the final tree can be thought of as a model with constant predictions in *square partitions* of the feature domain.  Hence, in building a tree, we need to address two questions:\n",
        "\n",
        "1. <span style=\"color:blue\"> How to come up with the splits that determine the square partitions? </span>\n",
        "\n",
        "2. <span style=\"color:blue\"> How to come up with the predictions for each of the square partitions? </span>\n",
        "\n",
        "The second task is straightforward:  We simply take the average of all predictions in the current partition.  \n",
        "\n",
        "For 2., one relies on a *greedy algorithm*: For each node, we go over all possible variables and all possible split levels, and then choose the combination of variable and split level that mimizes the prediction error.  However, note that the tree thus will generally not minimize the squared prediction error -- finding such a partition is computationally/algorithmically infeasible...\n",
        "\n",
        "### Excursion: The Sigmoid Function\n",
        "\n",
        "In generating our dataset we will use the so-called *sigmoid function*, which will be a key ingredient when we discuss *neural networks*.  It has the advantage that depending on the choice of a parameter, it can mimic a highly linear or a highly non-linear relationship:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\alpha}(x) = \\frac{1}{1+e^{-\\alpha\\,x}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "oFco9P3jzMD4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBGwtLkV7VRS"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return(1 / (1 + np.exp(-x)))\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.plot(x, sigmoid(x), color='b')\n",
        "plt.plot(x, sigmoid(0.5*x), color='r')\n",
        "plt.plot(x, sigmoid(5*x), color='g')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we notice that for a small choice of $\\alpha$ (red curve), we obtain a relationship that is almost linear (and, indeed, as $\\alpha$ becomes smaller and smaller, $\\sigma_{\\alpha}$ approaches a perfectly linear function).  Whereas for a larger choice of $\\alpha$ (green curve), we almost obtain a step function (and, indeed, as $\\alpha$ becomes larger and larger, $\\sigma_{\\alpha}$ approaches a step function).\n",
        "\n",
        "## A Simulated Example with One Predictor\n",
        "\n",
        "1. Let's start by generating two datasets based on the sigmoid function with different parameters taken from above."
      ],
      "metadata": {
        "id": "wyJW2guIzZ1U"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00OVN3o97VRT"
      },
      "source": [
        "np.random.seed(1) #Set a random seed for obtaining the same results every time we run the code\n",
        "x = 3 * np.random.normal(0, 1, 150)\n",
        "eps = 0.25 * np.random.normal(0, 1, 150)\n",
        "\n",
        "y_1 = sigmoid(0.5 * x) + eps\n",
        "y_2 = sigmoid(5 * x) + eps\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the first dataset:"
      ],
      "metadata": {
        "id": "HG-sriBRN7o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata1 = pd.DataFrame({'y_1':y_1,'x':x})\n",
        "mytraindata1 = mydata1[1:100]\n",
        "mytestdata1 = mydata1[101:150]\n",
        "\n",
        "plt.scatter(mydata1.x,mydata1.y_1)"
      ],
      "metadata": {
        "id": "12K6AWXPN9i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is the second dataset:"
      ],
      "metadata": {
        "id": "Oy7yZNtqOAC4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDvNPVwD7VRT"
      },
      "source": [
        "mydata2 = pd.DataFrame({'y_2':y_2,'x':x})\n",
        "mytraindata2 = mydata2[1:100]\n",
        "mytestdata2 = mydata2[101:150]\n",
        "plt.scatter(mydata2.x,mydata2.y_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. And, for comparison, let's fit a linear regression model to both datasets and let's evaluate the out-of-sample error."
      ],
      "metadata": {
        "id": "umVdXq4ozu7E"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPRQ8-T7VRT"
      },
      "source": [
        "X_1train = mytraindata1['x'].values.reshape(-1,1)\n",
        "X_1test = mytestdata1['x'].values.reshape(-1,1)\n",
        "y_1train = mytraindata1['y_1'].values\n",
        "lmfit1 = LinearRegression(fit_intercept=True)\n",
        "lmfit1.fit(X_1train,y_1train)\n",
        "yhat_OOS1 = lmfit1.predict(X_1test)\n",
        "OLS_OOS_MSE1 = sum((mytestdata1['y_1'] - yhat_OOS1)**2)/50\n",
        "OLS_OOS_MSE1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SEVVA_07VRU"
      },
      "source": [
        "X_2train = mytraindata2['x'].values.reshape(-1,1)\n",
        "X_2test = mytestdata2['x'].values.reshape(-1,1)\n",
        "y_2train = mytraindata2['y_2'].values\n",
        "lmfit2 = LinearRegression(fit_intercept=True)\n",
        "lmfit2.fit(X_2train,y_2train)\n",
        "yhat_OOS2 = lmfit2.predict(X_2test)\n",
        "OLS_OOS_MSE2 = sum((mytestdata2['y_2'] - yhat_OOS2)**2)/50\n",
        "OLS_OOS_MSE2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Let's fit a tree to the second dataset."
      ],
      "metadata": {
        "id": "09U5VR1Fz0ji"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nffD9ev7VRU"
      },
      "source": [
        "tree2 = DecisionTreeRegressor(max_leaf_nodes=3)\n",
        "tree2.fit(X_2train, y_2train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function creates images of tree models using pydot, as the package sklearn doesn't offer graphs of the trees\n"
      ],
      "metadata": {
        "id": "KpELVK_oz7DY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VpYt9zW7VRU"
      },
      "source": [
        "import pydot\n",
        "from IPython.display import Image\n",
        "def print_tree(estimator, features, class_names=None, filled=True):\n",
        "    tree = estimator\n",
        "    names = features\n",
        "    color = filled\n",
        "    classn = class_names\n",
        "\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)\n",
        "    graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
        "    return(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF0SN58g7VRU"
      },
      "source": [
        "graph, = print_tree(tree2, features=['x'])\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Let's use the test data and cross validation to see what size is optimal, and let's prune accordingly."
      ],
      "metadata": {
        "id": "NdRvpUV40KJ5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDmCteFY7VRV"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "OLS_OOS_MSE2 = []\n",
        "cv_score = []\n",
        "for i in range(2,6):\n",
        "    tree2 = DecisionTreeRegressor(max_leaf_nodes=i)\n",
        "    tree2.fit(X_2train, y_2train)\n",
        "    ytreehat2 = tree2.predict(X_2test)\n",
        "    OLS_OOS_MSE2.append(sum((mytestdata2['y_2'] - ytreehat2)**2)/50)\n",
        "    cv_score.append(sum(cross_val_score(tree2, X_2train, y_2train, cv=5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(range(2, 6), cv_score, marker='o')\n",
        "axes[0].set_title('Cross-Validation Score')\n",
        "axes[0].set_xlabel('Max Leaf Nodes')\n",
        "axes[0].set_ylabel('CV Score')\n",
        "\n",
        "axes[1].plot(range(2, 6), OLS_OOS_MSE2, marker='o', color='orange')\n",
        "axes[1].set_title('Out-of-Sample MSE')\n",
        "axes[1].set_xlabel('Max Leaf Nodes')\n",
        "axes[1].set_ylabel('MSE')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3nqBOi_tOkhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks that the simplest tree with only two leaves does best."
      ],
      "metadata": {
        "id": "5K0VDEJIPY1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree2 = DecisionTreeRegressor(max_leaf_nodes=2)\n",
        "tree2.fit(X_2train, y_2train)\n",
        "graph, = print_tree(tree2, features=['x'])\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "c_cpWggaP5IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytreehat2 = tree2.predict(X_2test)\n",
        "test_mse = mean_squared_error(mytestdata2['y_2'], ytreehat2)\n",
        "print(f\"Test MSE for tree2: {test_mse}\")"
      ],
      "metadata": {
        "id": "YvLDXbWeQyWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Let's redo for the first dataset."
      ],
      "metadata": {
        "id": "Le9_ijWW0iMk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7hunbTP7VRW"
      },
      "source": [
        "tree1 = DecisionTreeRegressor()\n",
        "tree1.fit(X_1train, y_1train)\n",
        "graph, = print_tree(tree1, features=['x'])\n",
        "Image(graph.create_png())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK9couni7VRW"
      },
      "source": [
        "OLS_OOS_MSE1 = []\n",
        "cv_score = []\n",
        "for i in range(2,7):\n",
        "    tree1 = DecisionTreeRegressor(max_leaf_nodes=i)\n",
        "    tree1.fit(X_1train, y_1train)\n",
        "    ytreehat1 = tree1.predict(X_1test)\n",
        "    OLS_OOS_MSE1.append(sum((mytestdata1['y_1'] - ytreehat1)**2)/50)\n",
        "    cv_score.append(sum(cross_val_score(tree1, X_1train, y_1train, cv=5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDGMecqORdJk"
      },
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(range(2, 7), cv_score, marker='o')\n",
        "axes[0].set_title('Cross-Validation Score')\n",
        "axes[0].set_xlabel('Max Leaf Nodes')\n",
        "axes[0].set_ylabel('CV Score')\n",
        "\n",
        "axes[1].plot(range(2, 7), OLS_OOS_MSE1, marker='o', color='orange')\n",
        "axes[1].set_title('Out-of-Sample MSE')\n",
        "axes[1].set_xlabel('Max Leaf Nodes')\n",
        "axes[1].set_ylabel('MSE')\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So ot looks like 4 leaves is optimal. Let's prune:"
      ],
      "metadata": {
        "id": "uvF5EefUQfpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree1 = DecisionTreeRegressor(max_leaf_nodes=4)\n",
        "tree1.fit(X_1train, y_1train)\n",
        "graph, = print_tree(tree1, features=['x'])\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "aU8JGe6iQnLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytreehat1 = tree1.predict(X_1test)\n",
        "test_mse = mean_squared_error(mytestdata1['y_1'], ytreehat1)\n",
        "print(f\"Test MSE for tree1: {test_mse}\")"
      ],
      "metadata": {
        "id": "kX-0mh-pQe-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the tree performs very differently between the two datasets:  For the second dataset, the tree provided a very simple (and understandable!) model of the data, and despite the simplicity the prediction accuracy improved relative to the linear regressions.  For the first dataset, on the other hand, the tree is far more complex than the regression model but still provides worse predictions.\n",
        "\n",
        "Hence, overall, trees are well-suited for modeling non-linear relationships -- which explains their relevance as the foundational structural element for more advanced learners."
      ],
      "metadata": {
        "id": "G6X4SK1D0u-p"
      }
    }
  ]
}