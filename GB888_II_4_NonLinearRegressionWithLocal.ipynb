{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_II_4_NonLinearRegressionWithLocal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Non-linear Regression Techniques\n",
        "\n",
        "This tutorial provides an introduction to local regression embedded in a simple illustration of non-linear regression more broadly. In particular, we will showcase a popular version: LOWESS (locally weighted scatterplot smoothing). This is often applied in low-dimensional settings (e.g., if there is only a single or only a few features) as a curve smoothing techique. So, essentially we are *letting the data tell us the shape of the curve*."
      ],
      "metadata": {
        "id": "JNGZbbCiRkR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will import scikit-lego for local regression, which is not one of the libraries that can be loaded in Colab."
      ],
      "metadata": {
        "id": "6xa9J22nR22l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-lego"
      ],
      "metadata": {
        "id": "nE9d5Kpg7Az7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's clone our git repository so as to have access to the data."
      ],
      "metadata": {
        "id": "2PRV7QtLSLCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7rMHNGajOvV"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's install some relevant libraries."
      ],
      "metadata": {
        "id": "DvC0N_VKSFaw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yZYbDPGZBq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import SplineTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklego.linear_model import LowessRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear Regression Techniques\n",
        "\n",
        "Non-linearities refer to sitiations where features enter the response variable $y$ not as a linear function. We had discussed non-linearities in the context of engineering features. Let's consider ways of accounting for non-linearities in the context of a single feature, i.e. if there is only one $x$.\n",
        "\n",
        "1. Use *transformations* of $x$: A traditional approach that falls in this category is *polynomial regression*, where we simply add powers of the feature $x$ -- i.e. $x^2$, $x^3$, etc. -- to the regression problem.  However, polynomials have some limitations, particularly in extremal areas, because they fit the regression function globally.  Instead, in *spline regression*, piecewise polynomials that are only fit between *knots* are used, but one imposes restrictions such that the fit is still continuous and smooth.  With so-called *natural splines*, a linear function is used in the extremal (corner) areas so as to avoid erratic behavior when extrapolating.  Depending on the number of knots, the function can mimic more or less arbitrary shapes.\n",
        "\n",
        "2. *Local regression*: Instead of using all points for predicting at a given point $x_0$, we run a *local* regression where we put more weight on the data points that are close to $x_0$ and less weight on data points that are far away.  The weighting is typically done via a so-called *kernel* function (generalized bell curve) so this is also referred to as *kernel regression*.\n"
      ],
      "metadata": {
        "id": "Ilfi7DvlSXsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example in the context of height-weight relationship\n",
        "\n",
        "We use a straightforward example: Regressing the weight of people on their heights. The relevant dataset is taken from a [the Davis data that we have seen before](https://rdrr.io/cran/carData/man/Davis.html):"
      ],
      "metadata": {
        "id": "iL_1gPeUSg9v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcUhqqpbGZBr"
      },
      "source": [
        "hwdata = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_IV_3_Davis.csv', index_col=0)\n",
        "hwdata = hwdata.sort_values('height')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "JoeN5uvIT6a-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQcuj4taGZBs"
      },
      "source": [
        "hwdata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's defined the $x$ and $y$ vectors:"
      ],
      "metadata": {
        "id": "EHWdL7HEUJwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = hwdata[['height']]\n",
        "y = hwdata['weight']"
      ],
      "metadata": {
        "id": "zENkLwAFUOmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with polynomial regression, where we are taking advantage of so-called *model pipelines* within scikit-learn, where we \"pipe\" the regression model into our linear regression model:"
      ],
      "metadata": {
        "id": "QN6VXn_OT8Uv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orzorYDJjuTO"
      },
      "source": [
        "polynomial_regression = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', LinearRegression(fit_intercept=False))])\n",
        "polynomial_regression = polynomial_regression.fit(X, y)\n",
        "polynomial_regression.named_steps['linear'].coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize via a scatter plot with polynomial regression line:"
      ],
      "metadata": {
        "id": "i4vks-QWUfpO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bk_XF8GZBt"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_poly = polynomial_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='g', label='polynomial regression df=4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we may be overfitting a bit...\n",
        "\n",
        "Let's look at regression splines, again using a model pipeline (we get splines via 'SplineTransformer'):"
      ],
      "metadata": {
        "id": "vJ4RsocQUoEd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2thG-dVqZFt"
      },
      "source": [
        "spline_regression = Pipeline([('splines', SplineTransformer(degree=2, n_knots=3, extrapolation='linear')),('linear', LinearRegression(fit_intercept=False))])\n",
        "spline_regression = spline_regression.fit(X, y)\n",
        "spline_regression.named_steps['linear'].coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a spline regression line:"
      ],
      "metadata": {
        "id": "gdHCaKuLU91L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Vh7s5qvNfg"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_splines = spline_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_splines, color='g', label='Spline regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "Finally, let's run a local regression, where we rely on 'LowessRegression' from scikit-lego:"
      ],
      "metadata": {
        "id": "4Llw3Eg6VGY7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIdO1yQYv6gN"
      },
      "source": [
        "local_regression = LowessRegression(sigma=50).fit(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a regression line:"
      ],
      "metadata": {
        "id": "hlt2l-X-VR8b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5WlCKnxSHL"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_local = local_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_local, color='g', label='local regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "Let's summarize by plotting altogether:"
      ],
      "metadata": {
        "id": "YML989SFVZZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AoJBu4_N4kIT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znbTtt-dGZBx"
      },
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='r', label='polynomial regression df=4')\n",
        "plt.plot(hwdata.height, pred_splines, color='b', label='Natural spline df=4')\n",
        "plt.plot(hwdata.height, pred_local, color='y', label='Local regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the moral of the story is that these techniques present very neat tools for modeling non-linear relationships. And, importantly, the **data tell us what the shape of the curve ought to be**!"
      ],
      "metadata": {
        "id": "fL9rlSgI4jkP"
      }
    }
  ]
}