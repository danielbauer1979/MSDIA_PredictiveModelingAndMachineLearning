{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB888_IV_4_Bagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging\n",
        "\n",
        "In this tutorial, we discuss approaches to improve on the predictive porformance of CARTs via *aggregation*. We consider a basic bootstrap aggregation approach in the context of a simple example with a single predictor, illustrating key aspects and pitfalls.\n",
        "\n",
        "As usually, let's start with loading the relevant libaries."
      ],
      "metadata": {
        "id": "_Dgf3D6YiA0C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's define:"
      ],
      "metadata": {
        "id": "CIxS7A-w4GGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return(1 / (1 + np.exp(-x)))"
      ],
      "metadata": {
        "id": "QGnILMfU4HaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improvement via Aggregation\n",
        "\n",
        "### Review of Concepts and Maths\n",
        "\n",
        "As we have discussed in GB 656, there is a key tradeoff between *bias* and *variance*:  A more complex learner may perform marvelously *in-sample*, but the *out-of-sample* performance is poor due to *overfitting*.  *Ensemble* learning techniques use the input from basic learners trained on different data sets (or also the input from different learners trained on the same data set -- this is referred to as *stacking*), to improve on the predictive performance of the basic learner.\n",
        "\n",
        "In *Bagging* -- short for *bootstrap-aggregating* -- the idea is to sample from the original dataset to obtain $M$ bootstrap samples, which are in turn used for training the predictive model yielding predictions $\\hat{Y}_j,$ $j=1,\\ldots,M.$  The final prediction $\\hat{Y}$ then is based on the average of the different predictions: $\\hat{Y} = \\frac{1}{M} \\sum_{j=1}^M \\hat{Y}_j$.  \n",
        "\n",
        "For instance, if each of the $M$ predictions is *unbiased*, $\\mathbb{E}_x[\\hat{Y}_i] = Y,$ then of course the aggregated prediction will be unbiased as well: $\\mathbb{E}_x[\\hat{Y}] = Y.$ However, we generally have for the standard deviation:\n",
        "$$\n",
        "\\text{StDev}_x(\\hat{Y}) = \\frac{1}{N} \\text{StDev}_x\\left(\\sum_{j=1}^M \\hat{Y}_j\\right) \\leq \\frac{1}{N} \\sum_{j=1}^M \\text{StDev}_x(Y_j),\n",
        "$$\n",
        "where the inequality is sharp if the predictions are not perfectly positively correlated.  Hence, aggregating can reduce the variance!\n",
        "\n",
        "### A Simulated Example with One Predictor\n",
        "\n",
        "Let us revisit the simple example from the previous tutorial on trees.  As a reminder, there we used the so-called *sigmoid* function that can depict highly linear as well as highly non-linear relationships by different choices of its parameter.  We used different parameters to generate two data sets, and compared the predictive preformance of trees vs. that of OLS regression.  Our conclusion was that trees work well in the non-linear situation whereas (linear) regression works well in the linear situation:\n"
      ],
      "metadata": {
        "id": "oFco9P3jzMD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Generate the datasets:"
      ],
      "metadata": {
        "id": "7jE6cihh4bpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "x = 3 * np.random.normal(0, 1, 150)\n",
        "eps = 0.25 * np.random.normal(0, 1, 150)\n",
        "y_1 = sigmoid(0.5 * x) + eps\n",
        "y_2 = sigmoid(5 * x) + eps\n",
        "mydata1 = pd.DataFrame({'y_1':y_1,'x':x})\n",
        "mytraindata1 = mydata1[0:100]\n",
        "mytestdata1 = mydata1[100:150]\n",
        "mydata2 = pd.DataFrame({'y_2':y_2,'x':x})\n",
        "mytraindata2 = mydata2[0:100]\n",
        "mytestdata2 = mydata2[100:150]\n"
      ],
      "metadata": {
        "id": "8ijLyBLP4Sgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fit an OLS regression to the first dataset:"
      ],
      "metadata": {
        "id": "o9WH3oR_4Q1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmfit1 = smf.ols(formula=\"y_1 ~ x\", data=mytraindata1).fit()\n",
        "yhat_OOS1 = lmfit1.predict(mytestdata1)\n",
        "OLS_OOS_MSE1 = sum((mytestdata1['y_1'] - yhat_OOS1)**2)/50\n",
        "OLS_OOS_MSE1"
      ],
      "metadata": {
        "id": "1xqsIIJL4hXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fit a tree:"
      ],
      "metadata": {
        "id": "M3ZE0KNN4Rq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " tree1 = DecisionTreeRegressor(max_leaf_nodes=2)\n",
        "X = mytraindata1['x'].values.reshape(-1, 1)\n",
        "y = mytraindata1['y_1'].values\n",
        "tree1.fit(X, y)\n",
        "ytreehat1 = tree1.predict(mytestdata1['x'].values.reshape(-1, 1))\n",
        "TREE_OOS_MSE1 = sum((mytestdata1['y_1'] - ytreehat1)**2)/50\n",
        "TREE_OOS_MSE1"
      ],
      "metadata": {
        "id": "2UstDPtB5DEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, rather than generating one tree, let's contemplate an alternative.  Let us draw new data sets from sampling from the original data set (*bootstrapping*), let's fit an (unpruned) tree to each of the sampled data sets, and let's predict by averaging over the predictions of these new trees."
      ],
      "metadata": {
        "id": "cBNwtI1-5E76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ybaggedtreehat1 = np.zeros(mytestdata1['y_1'].shape)\n",
        "atree = DecisionTreeRegressor()\n",
        "for i in range(0, 100):\n",
        "  subset = np.random.choice(len(mytraindata1), 25, replace=True)\n",
        "  X = mytraindata1['x'][subset].values.reshape(-1, 1)\n",
        "  y = mytraindata1['y_1'][subset].values\n",
        "  atree.fit(X, y)\n",
        "  ybaggedtreehat1 = ybaggedtreehat1 + atree.predict(mytestdata1['x'].values.reshape(-1, 1))\n",
        "ybaggedtreehat1 = ybaggedtreehat1/100\n",
        "BAGGED_MSE1 = sum((mytestdata1['y_1'] - ybaggedtreehat1)**2)/50\n",
        "BAGGED_MSE1"
      ],
      "metadata": {
        "id": "t_cfdEX85FOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that by aggregating, the tree-based predictions perform notably better than the single tree.  What is going on?  There are two aspects worth noting:\n",
        "\n",
        "1. As explained above, by aggregating the individual trees we potentially reduce the variance of the prediction.\n",
        "\n",
        "2. We are not pruning the individual trees which leads to lower bias.  While this may lead to overfitting by any individual tree, we control the variance by subsequently averaging.  Thus,  we potentially reduce the bias of the prediction.\n",
        "\n",
        "Let's compare the predictions:"
      ],
      "metadata": {
        "id": "iTOoQgZK4RYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(mytestdata1['x'], mytestdata1['y_1'], c = 'k')\n",
        "plt.plot(mytestdata1['x'], yhat_OOS1, c = 'k')\n",
        "plt.scatter(mytestdata1['x'], ytreehat1, c = 'r')\n",
        "plt.scatter(mytestdata1['x'], ybaggedtreehat1, c = 'b')"
      ],
      "metadata": {
        "id": "xRRtyuyZ5lBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also try the second dataset:\n",
        "\n",
        "1. Tree:"
      ],
      "metadata": {
        "id": "JdD4221k5j99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree2 = DecisionTreeRegressor(max_leaf_nodes=2)\n",
        "X = mytraindata2['x'].values.reshape(-1, 1)\n",
        "y = mytraindata2['y_2'].values\n",
        "tree2.fit(X, y)\n",
        "ytreehat2 = tree2.predict(mytestdata2['x'].values.reshape(-1, 1))\n",
        "TREE_OOS_MSE2 = sum((mytestdata2['y_2'] - ytreehat2)**2)/50\n",
        "TREE_OOS_MSE2"
      ],
      "metadata": {
        "id": "fw0OPS2C66PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Bagging:"
      ],
      "metadata": {
        "id": "iHBjbElU5jwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ybaggedtreehat2 = np.zeros(mytestdata2['y_2'].shape)\n",
        "atree = DecisionTreeRegressor()\n",
        "for i in range(0, 100):\n",
        "  subset = np.random.choice(len(mytraindata2), 30, replace=True)\n",
        "  X = mytraindata2['x'][subset].values.reshape(-1, 1)\n",
        "  y = mytraindata2['y_2'][subset].values\n",
        "  atree.fit(X, y)\n",
        "  ybaggedtreehat2 = ybaggedtreehat2 + atree.predict(mytestdata2['x'].values.reshape(-1, 1))\n",
        "ybaggedtreehat2 = ybaggedtreehat2/100\n",
        "BAGGED_MSE2 = sum((mytestdata2['y_2'] - ybaggedtreehat2)**2)/50\n",
        "BAGGED_MSE2"
      ],
      "metadata": {
        "id": "YBWlLq5U7CyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it turns out here the aggregated sample beats even the basic tree model, which performed quite well.  But why?  Let's look at the predictions:"
      ],
      "metadata": {
        "id": "yrofpLI-7WBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmfit2 = smf.ols(formula=\"y_2 ~ x\", data=mytraindata2).fit()\n",
        "yhat_OOS2 = lmfit2.predict(mytestdata2)\n",
        "plt.scatter(mytestdata2['x'], mytestdata2['y_2'], c = 'k')\n",
        "plt.plot(mytestdata2['x'], yhat_OOS2, c = 'k')\n",
        "plt.scatter(mytestdata2['x'], ytreehat2, c = 'r')\n",
        "plt.scatter(mytestdata2['x'], ybaggedtreehat2, c = 'b')"
      ],
      "metadata": {
        "id": "UdnpfKG75jdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in the extremal areas, the predictions are similar, but the predictions of the aggregated estimator are smooth around the cutoff area.  A single tree that has one precise cutoff is likely to get it (slightly) wrong, so the smoothed transition (reflecting some ambiguity of whether the points in this area should be \"up\" or \"down\") generally performs better.\n",
        "\n",
        "Now contemplate the (more realistic) situation where it is not ex ante clear of whether the relationship is more linear or more non-linear.  In this case, the aggregated estimator may be a conservative choice -- and it appears to definitely outperform the tree!"
      ],
      "metadata": {
        "id": "TOGZ0MLE8i8A"
      }
    }
  ]
}