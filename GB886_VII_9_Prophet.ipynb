{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTeJG35xqEinvbCO8ZfTuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning/blob/main/GB886_VII_9_Prophet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Univariate Time Series\n",
        "\n",
        "This codebook goes through a few time series examples to showcase basics of univariate time series modeling for generating forecasts. We first look at some basic examples where we carry the steps out manually. We then show how packages and particularly Meta's Prophet can automate the analysis.\n"
      ],
      "metadata": {
        "id": "eHzhhWxB-V0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usually, let's load relevant libraries where we rely on time series functionality in statsmodels for our analysis"
      ],
      "metadata": {
        "id": "ZsyJD5FgCLpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Time series functionality\n",
        "from statsmodels.tsa.stattools import adfuller #ADF test for stationarity\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "#Prophen by Meta, https://facebook.github.io/prophet\n",
        "from prophet import Prophet\n",
        "\n",
        "#For calculating error metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ],
      "metadata": {
        "id": "jPbPVkBbrYXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two Examples\n",
        "\n",
        "We will go through two introductory examples to demonstrate different types of time series: We look at the S&P500 stock index (daily observations) as one example, and an example of retail sales (monthly observations) as a second example.\n",
        "\n",
        "### Retail Sales Data\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "aMSzijK4C7TH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz-E_E6MqM_J"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/danielbauer1979/MSDIA_PredictiveModelingAndMachineLearning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dat_sales = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_VII_3_RetailSales.csv')\n",
        "dat_sales.tail()"
      ],
      "metadata": {
        "id": "qBuKt_rzrNe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data goes from January 1992 (index zero) to May 2016 (last index). Let's plot it:"
      ],
      "metadata": {
        "id": "40yea-btGwNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dat_sales)\n",
        "plt.title('Retail Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d23mEVuGw9vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is from **visual inspection** [link text](https://)evident that there is seasonality---the same pattern seems to occur over and over again. This is no suprise for retail sales (increased sales around the holidays, say), so our intuition is also helpful. Furthermore, there seems to be a time trend (the series trends upwards).\n",
        "\n",
        "So, to remove the trend, we use the \"decomposition\" functionality in statsmodels:"
      ],
      "metadata": {
        "id": "S4oAze5kHCh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform seasonal decomposition\n",
        "result = seasonal_decompose(dat_sales, model='additive', period=12)\n",
        "\n",
        "# Plot the trend component\n",
        "plt.plot(result.trend)\n",
        "plt.title('Trend Component of Retail Sales')\n",
        "plt.show()\n",
        "\n",
        "#Plot the seasonal component\n",
        "plt.plot(result.seasonal)\n",
        "plt.title('Seasonal Component of Retail Sales')\n",
        "plt.show()\n",
        "\n",
        "# Plot the residual component\n",
        "plt.plot(result.resid)\n",
        "plt.title('Residual Component of Retail Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9jwr21PtmC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice the clear trend and the seasonal pattern (which keeps on repeating). Let's check whether the residual is stationary using the *Augmented Dickey Fuller (ADF) test*, where the Null hypothesis is *non-stationarity*---so, if the p-value is small and we reject, there is evidence for stationarity:"
      ],
      "metadata": {
        "id": "GRzs5JhnHzcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_adf = adfuller(result.resid.dropna())\n",
        "print('ADF Statistic: %f' % result_adf[0])\n",
        "print('p-value: %f' % result_adf[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result_adf[4].items():\n",
        "\tprint('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "yeIK0_Tau0FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, it appears that the ADF test rejects, so it seems after removing trend and seasonality, the data appears stationary!"
      ],
      "metadata": {
        "id": "UD1uD930KLdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate the autocorrelation function (ACF) and the partial autocorrelation function (PACF). As reminder, under the Box-Jenkins approach, inpsection of the ACF and the PACF are key to modeling the stationary part."
      ],
      "metadata": {
        "id": "T8SjmtuD2cVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf(result.resid.dropna(), lags=20)\n",
        "plt.title('ACF of Residuals')\n",
        "plt.show()\n",
        "\n",
        "plot_pacf(result.resid.dropna(), lags=20)\n",
        "plt.title('PACF of Residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "njkF1mtwef08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we see some significant spikes in both the ACF and the PACF until lag 12. the spikes in the PACF seem more subsantive. There are several models that may be possible. We could opt for an ARMA(12,12), which is appropriate for decays after 12 lags on both ACF and PACF. Another option may be an AR(12) because we see significant spikes in the PACF.\n",
        "\n",
        "Let's check both options:"
      ],
      "metadata": {
        "id": "3pm9-lIF3Mbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_12_12 = ARIMA(result.resid.dropna(), order=(12, 0, 12))\n",
        "results_12_12 = model_12_12.fit()\n",
        "print(results_12_12.summary())"
      ],
      "metadata": {
        "id": "hTmn3Kmi4FK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a convegrence warning, saying that the covariance matrix is close to singular. This means we should be careful in interpreting the standard errors, but we will ignore the convergence warnings for now."
      ],
      "metadata": {
        "id": "L1UvAvdUdAJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ar12 = ARIMA(result.resid.dropna(), order=(12, 0, 0))\n",
        "results_ar12 = model_ar12.fit()\n",
        "print(results_ar12.summary())"
      ],
      "metadata": {
        "id": "KE6Jyq55ih5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ljung-Box Q test that checks for any remaining autocorrelation rules out remaining auto-correlation in the ARMA(12,12) model (the metric is zero) but there still seems to be some remaining autocorrelation on the AR(12). However, the JB test suggest that the AR(12) residuals look more Normal.\n",
        "\n",
        "Let's run with the ARMA(12,12) model. To validate, let's look at the residuals:"
      ],
      "metadata": {
        "id": "zrUIjpHo5UMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "residuals_12_12 = results_12_12.resid\n",
        "\n",
        "# Plot the residuals\n",
        "plt.plot(residuals_12_12)\n",
        "plt.title('Residuals of ARMA(12, 12) Model')\n",
        "plt.show()\n",
        "\n",
        "# Plot the histogram of residuals\n",
        "plt.hist(residuals_12_12)\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.show()\n",
        "\n",
        "# Plot the ACF of residuals\n",
        "plot_acf(residuals_12_12, lags=20)\n",
        "plt.title('ACF of Residuals (ARMA(12, 12))')\n",
        "plt.show()\n",
        "\n",
        "# Plot the PACF of residuals\n",
        "plot_pacf(residuals_12_12, lags=20)\n",
        "plt.title('PACF of Residuals (ARMA(12, 12))')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YJSS5xQo5x2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So they look reasonably like **white noise**: The realizations look roughly Normal and the ACF and PACF don't show any residual autocorrelations."
      ],
      "metadata": {
        "id": "d9eRo05Z61xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's appraise the **performance of the forecasts** using this model. In doing so, we split the full time seroes into two windows: the last 24 data points as the validation set and then the remaining data points as the training data set. We will use the training set to fit our model, and then check how well it performs relative to our validation data (however, before we use the model in our applications, we would (re-)estimate it on the full data as we did above):"
      ],
      "metadata": {
        "id": "vXaiyywqauM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = dat_sales[:-24]\n",
        "validation_data = dat_sales[-24:]"
      ],
      "metadata": {
        "id": "8LXvkn26YrpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's obtain the trend and seasonality component by decomposing the train_data. And then we fit our ARMA(12,12) model on the residuals of train_data:"
      ],
      "metadata": {
        "id": "YvxfNHnZbWvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decomp_train = seasonal_decompose(train_data, model='additive', period=12, extrapolate_trend='freq')\n",
        "\n",
        "armamodel_train = ARIMA(decomp_train.resid.dropna(), order=(12, 0, 12))\n",
        "fittedarma_train = armamodel_train.fit()"
      ],
      "metadata": {
        "id": "QFFubIhKZAtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will forecast the trend component by using the difference between the most recent trend components and extrapolating that to the future (i.e., $trend(T+\\tau)=trend(T-1) + (\\tau+1) \\times [ trend(T) - trend(T-1)]$)."
      ],
      "metadata": {
        "id": "p30zZz2jcyp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_trend = decomp_train.trend.iloc[-1] + np.arange(1, 25) * decomp_train.trend.diff().iloc[-1]"
      ],
      "metadata": {
        "id": "TFV1JychdXyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we just use the past 12 monyhs to forecast for the next 24 months (we repeat twice):"
      ],
      "metadata": {
        "id": "bvejpAtRjO2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_seasonal = decomp_train.seasonal[-12:].values\n",
        "forecast_seasonal = np.tile(forecast_seasonal, 2)"
      ],
      "metadata": {
        "id": "pEE-tO6rdebw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we forecast our ARMA(12,12) model and combine the three components plus the confidence interval from the ARMA model to get our total forecasts:"
      ],
      "metadata": {
        "id": "6DrlnoPkjY7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_arma = fittedarma_train.predict(start=len(decomp_train.resid.dropna()), end=len(decomp_train.resid.dropna()) + 23)\n",
        "forecast = forecast_trend + forecast_seasonal[:24] + forecast_arma.values\n",
        "\n",
        "forecast_arma_ci = fittedarma_train.get_forecast(steps=24).conf_int()\n",
        "forecast_lower = forecast_trend + forecast_seasonal[:24] + forecast_arma_ci.iloc[:, 0].values\n",
        "forecast_upper = forecast_trend + forecast_seasonal[:24] + forecast_arma_ci.iloc[:, 1].values\n",
        "\n",
        "# Create a DataFrame for the forecast with confidence intervals\n",
        "forecast_df = pd.DataFrame({'Forecast': forecast,\n",
        "                           'Lower Bound': forecast_lower,\n",
        "                           'Upper Bound': forecast_upper},\n",
        "                          index=validation_data.index)"
      ],
      "metadata": {
        "id": "EgmahQ_we6Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are our forecasts:"
      ],
      "metadata": {
        "id": "nGvLrK2fkNxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_df['Actual'] = validation_data.values"
      ],
      "metadata": {
        "id": "IxvxIpp7kgtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecast_df"
      ],
      "metadata": {
        "id": "GW53weQFfQp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the confidence inteval seems to include the actuals most of the time, and that the forecasts are close to the actuals!"
      ],
      "metadata": {
        "id": "05a90OAhku0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate a plot of the series, the validation set, our forecasts, and the validation data:"
      ],
      "metadata": {
        "id": "z2_iWMWmkRTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_data, label='Training Data')\n",
        "plt.plot(validation_data, label='Actual Values')\n",
        "plt.plot(forecast_df['Forecast'], label='Forecast')\n",
        "plt.fill_between(forecast_df.index, forecast_df['Lower Bound'], forecast_df['Upper Bound'], color='gray', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title('Retail Sales Forecast')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZTbXTE9nd9_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, finally, let's calculate some error metrics:"
      ],
      "metadata": {
        "id": "EzgRF5-GlMPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(validation_data, forecast_df['Forecast']))\n",
        "print('RMSE:', rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(validation_data, forecast_df['Forecast'])\n",
        "print('MAE:', mae)\n",
        "\n",
        "# Calculate MAPE\n",
        "mape = np.mean(np.abs((validation_data.values.flatten() - forecast_df['Forecast'].values))/validation_data.values.flatten()) * 100\n",
        "print('MAPE:', mape)\n"
      ],
      "metadata": {
        "id": "cntEG2nlZ_tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty good!"
      ],
      "metadata": {
        "id": "2Or8vYfXldhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an alternative, we consider using Facebook's **Prophet** Time Series toolbox for the same data. The data need to be in a specific format, with date ('ds') and variable ('y') columns. We can obtain this from the Prophet github site, where we got this data from in the first place:"
      ],
      "metadata": {
        "id": "8b80e8FZ0VOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat_sales = pd.read_csv('https://raw.githubusercontent.com/facebook/prophet/main/examples/example_retail_sales.csv')\n",
        "dat_sales"
      ],
      "metadata": {
        "id": "0GIsWwgOrUg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will model the data as a time series object in Prophet:"
      ],
      "metadata": {
        "id": "eZmi444PsIbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_series = Prophet()\n",
        "time_series.fit(dat_sales)"
      ],
      "metadata": {
        "id": "rJlUBGfMrCmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, to start with, we will forecast over the next 24 months:"
      ],
      "metadata": {
        "id": "TsmhcynX08BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "future = time_series.make_future_dataframe(periods=24,freq='MS')\n",
        "future.tail()"
      ],
      "metadata": {
        "id": "N4N0j2UGsNJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate predictions:"
      ],
      "metadata": {
        "id": "0g2ddbEo1DPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = time_series.predict(future)\n",
        "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ],
      "metadata": {
        "id": "z_8p4vvxsc9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These seem fairly similar to our \"manual\" predictions. Let's visualize:"
      ],
      "metadata": {
        "id": "KjY9cfOr1G1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_fig_1 = time_series.plot(forecast)"
      ],
      "metadata": {
        "id": "yBvg096LtaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the components:"
      ],
      "metadata": {
        "id": "Zwg88Aj81Qro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prophet_fig_2 = time_series.plot_components(forecast)"
      ],
      "metadata": {
        "id": "VjVHN3M7toEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To illustrate how these forecasts came about, let's look into some of the detail. For instance, let's look at the change points that are incorporated in the trend component:"
      ],
      "metadata": {
        "id": "H3Vwwv2p1qy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet.plot import add_changepoints_to_plot\n",
        "fig = time_series.plot(forecast)\n",
        "a = add_changepoints_to_plot(fig.gca(), time_series, forecast)"
      ],
      "metadata": {
        "id": "5mywlAjFufMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we notice a number of change points that are included.\n",
        "\n",
        "And let's check the preformance by evalualting some out-of-sample error metrics. We will use cross validation, which is automated in Prophet (although Prophet operates using daily data):"
      ],
      "metadata": {
        "id": "hmgYUuDU19Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet.diagnostics import cross_validation\n",
        "df_cv = cross_validation(time_series, initial='1095 days', period='365 days', horizon = '730 days')\n",
        "#You can adjust the windows---here we use three years initially and then various windows thereafter"
      ],
      "metadata": {
        "id": "C1w8iPSLzT5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate the perfomance:"
      ],
      "metadata": {
        "id": "UEw1kvE62uB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet.diagnostics import performance_metrics\n",
        "df_p = performance_metrics(df_cv)\n",
        "df_p.tail()"
      ],
      "metadata": {
        "id": "zLldP9-U0GJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So using the various windows, we notice that the errors are a bit larger than in our validation using the single hold-out window. Let's visualize:"
      ],
      "metadata": {
        "id": "TUx7o5nM2_ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet.plot import plot_cross_validation_metric\n",
        "fig = plot_cross_validation_metric(df_cv, metric='mape')"
      ],
      "metadata": {
        "id": "saKCzMdRzpd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the further we forecast, the larger the error---which makes sense. But a MAPE of 10% is still pretty good."
      ],
      "metadata": {
        "id": "Ev4llIzF3pjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### S&P 500 Daily Data\n",
        "\n",
        "Let's now load and look at the stock data:"
      ],
      "metadata": {
        "id": "AMLLPVslGpy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat_sandp = pd.read_csv('MSDIA_PredictiveModelingAndMachineLearning/GB886_VII_3_SanP500.csv')\n",
        "dat_sandp.head()"
      ],
      "metadata": {
        "id": "9R7oBitOrFcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is from 2/28/2023 (index zero) until 2/28/2024 (last index). Let's take a quick look:"
      ],
      "metadata": {
        "id": "hCdgIXO2Dz3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dat_sandp)\n",
        "plt.title('Evolution of S and P Index 3/2023 to 3/2024')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IuyrVTx0rf3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the first question is to check whether the time series is stationary. Here, **visual inspection** again makes it clear that the data appears not stationary, although there is also not a very clear deterministic trend.\n",
        "\n",
        "Let's check the ADF test:"
      ],
      "metadata": {
        "id": "KU53NhHrr5TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = adfuller(dat_sandp)\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "\tprint('\\t%s: %.3f' % (key, value))\n"
      ],
      "metadata": {
        "id": "N0lM46igsCJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, here the ADF test does not reject so we **do not** have stationarity. So, let's take the **first differences**, i.e., let's look at\n",
        "$$\n",
        "y^d_t = y_t - y_{t-1}\n",
        "$$\n",
        "and then work with the differenced series $\\{y^d_1,y^d_2,y^d_3,...\\}$:"
      ],
      "metadata": {
        "id": "ISbjp5pDK9pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat_sandp_diff = dat_sandp.diff().dropna()\n",
        "plt.plot(dat_sandp_diff)\n",
        "plt.title('First Differences of S and P Index 3/2023 to 3/2024')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ed1LkZ8OtF5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, there is no longer an indication of a trend. Let's run the ADF test on the differenced series."
      ],
      "metadata": {
        "id": "_3vAQYp0Lhf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = adfuller(dat_sandp_diff)\n",
        "print('ADF Statistic: %f' % result[0])\n",
        "print('p-value: %f' % result[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result[4].items():\n",
        "\tprint('\\t%s: %.3f' % (key, value))"
      ],
      "metadata": {
        "id": "KO6BgDj_tO7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, that looks stationary. So, we can continue with our anaylsis using the differenced series."
      ],
      "metadata": {
        "id": "V758w3TULy5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the ACF and the PACF:"
      ],
      "metadata": {
        "id": "-MYfuSEA7XH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_acf(dat_sandp_diff, lags=20)\n",
        "plt.title('ACF of Differenced Series')\n",
        "plt.show()\n",
        "\n",
        "plot_pacf(dat_sandp_diff, lags=20)\n",
        "plt.title('PACF of Differenced Series')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qQaYsRRh7hwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we don't detect much correlation or autocorrelation. Let's look at a histogram:"
      ],
      "metadata": {
        "id": "_q5kJw887xl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(dat_sandp_diff)\n",
        "plt.title('Histogram of Differenced Series')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SeYDOx4J9vqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, all in all, it seems reasonable to assume that the series is white noise. Let's look at the mean and variance:"
      ],
      "metadata": {
        "id": "DouJVnPg9ra8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_diff = dat_sandp_diff.mean()\n",
        "std_diff = dat_sandp_diff.std()\n",
        "\n",
        "# Calculate the standard error of the mean\n",
        "n = len(dat_sandp_diff)\n",
        "se_mean = std_diff / np.sqrt(n)\n",
        "\n",
        "# Calculate the 95% confidence interval for the mean\n",
        "lower_mean = mean_diff - 1.96 * se_mean\n",
        "upper_mean = mean_diff + 1.96 * se_mean\n",
        "\n",
        "print(\"Point Estimate for Mean:\", mean_diff.iloc[0])\n",
        "print(\"95% Confidence Interval for Mean:\", (lower_mean.iloc[0], upper_mean.iloc[0]))\n",
        "print(\"Standard Deviation:\", std_diff.iloc[0])"
      ],
      "metadata": {
        "id": "8Gl4dvME9TEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the mean is significantly different from zero. Our final model for the series is:\n",
        "$$\n",
        "S_t = S_{t-1} + 4.45836 + 33.8281 \\times \\varepsilon_t,\\;\\;\\varepsilon_t\\sim N(0,1)\n",
        "$$\n",
        "Such a time series is called a **random walk with drift**."
      ],
      "metadata": {
        "id": "77gZdnHB9SvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's **simulate** some realizations going forward. When simulating, i.e., drawing random realizations, it is always a good idea to set a so-called *random seed* to make the results reproducable. That is, the next time we run the simulation, we want to draw the same random numbers:"
      ],
      "metadata": {
        "id": "YziL0KIwllC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(86)"
      ],
      "metadata": {
        "id": "pM7QXcJwiCUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate 20 simulated paths starting at the endnpoint, where we will similate 250 days forward. We use our random walk with drift model for the simulation:"
      ],
      "metadata": {
        "id": "Xeu8aT5omwWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_simulations = 20\n",
        "num_steps = 250\n",
        "\n",
        "simulated_series = []\n",
        "for j in range(num_simulations):\n",
        "  series = [dat_sandp.iloc[-1, 0]]  # Start with the last value of the original series\n",
        "  for j in range(num_steps):\n",
        "    drift = mean_diff.iloc[0]\n",
        "    shock = np.random.normal(0, std_diff.iloc[0])\n",
        "    new_value = series[-1] + drift + shock\n",
        "    series.append(new_value)\n",
        "  simulated_series.append(series)"
      ],
      "metadata": {
        "id": "AT_YIWcbgwfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize by plotting our original series and then appending our different **scenarios** at the end:\n",
        "\n"
      ],
      "metadata": {
        "id": "2IQymT7RnHII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(dat_sandp.index, dat_sandp.values, label='Original Series')\n",
        "\n",
        "for series in simulated_series:\n",
        "  plt.plot(np.arange(dat_sandp.index[-1]+1, dat_sandp.index[-1]+1+len(series)), series, alpha=0.5)\n",
        "\n",
        "plt.title('Simulated Random Walks with Drift')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CCt1PVi7nPha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we see under this model the possible values for the S&P 500 going forward are fairly wide!"
      ],
      "metadata": {
        "id": "Yh7J879znVZj"
      }
    }
  ]
}